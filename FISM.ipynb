{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\administrator\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:13: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras import Model\n",
    "import pickle\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import os\n",
    "import random\n",
    "from time import time\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class DataSet:\n",
    "\n",
    "    def __init__(self, train_file, test_file, negative_sample=3, batch_size=100):\n",
    "        self.train_data = pd.read_csv(train_file)\n",
    "        self.test_data = pd.read_csv(test_file)\n",
    "        self.num_users = self.train_data['userID'].max() + 1\n",
    "        self.num_items = self.train_data['itemID'].max() + 1\n",
    "        self.negative_sample = negative_sample\n",
    "        self.batch_size = batch_size\n",
    "        self.user_rated_items = self.get_user_rated_items()\n",
    "\n",
    "    def get_user_rated_items(self):\n",
    "        rated_data = self.get_rated_data()\n",
    "        user_rated_items = dict()\n",
    "        for user_id, item_id, rate in rated_data:\n",
    "            if not user_rated_items.__contains__(user_id):\n",
    "                user_rated_items[user_id] = []\n",
    "            user_rated_items[user_id].append(item_id)\n",
    "        return user_rated_items\n",
    "\n",
    "    def generate_train_data(self):\n",
    "        rated_data = self.get_rated_data()\n",
    "#         print('done load data')\n",
    "        np.random.shuffle(rated_data)\n",
    "        all_train_data = self.negative_sampling(rated_data)\n",
    "        all_batch_data = self.get_all_batch_data(all_train_data, self.user_rated_items)\n",
    "        return all_batch_data\n",
    "\n",
    "    def get_rated_data(self):\n",
    "        return [(user_id, item_id, rate) for user_id, item_id, rate in self.train_data[['userID', 'itemID', 'rating']].values]\n",
    "\n",
    "    def negative_sampling(self, rated_data):\n",
    "        user_ids = []\n",
    "        item_ids = []\n",
    "        labels = []\n",
    "        set_rated = {(user_id, item_id) for user_id, item_id, rate in rated_data}\n",
    "        for user_id, item_id, rate in rated_data:\n",
    "            user_ids.append(user_id)\n",
    "            item_ids.append(item_id)\n",
    "            labels.append((1, rate))\n",
    "            for j in range(self.negative_sample):\n",
    "                random_item = np.random.randint(self.num_items)\n",
    "                while set_rated.__contains__((user_id, random_item)):\n",
    "                    random_item = np.random.randint(self.num_items)\n",
    "                user_ids.append(user_id)\n",
    "                item_ids.append(random_item)\n",
    "                labels.append((0, 0))\n",
    "        return user_ids, item_ids, labels\n",
    "\n",
    "    def get_all_batch_data(self, all_train_data, user_rated_items):\n",
    "        user_ids, item_ids, labels = all_train_data\n",
    "        mask = self.num_items\n",
    "        num_iter = user_ids.__len__()//self.batch_size\n",
    "        batch_data = []\n",
    "        for i in range(num_iter):\n",
    "            batch_user_descriptions = []\n",
    "            batch_item_ids = []\n",
    "            batch_user_ids = []\n",
    "            batch_num_items = []\n",
    "            batch_labels = []\n",
    "            batch_ratings = []\n",
    "            for j in range(self.batch_size):\n",
    "                idx = i * self.batch_size + j\n",
    "                user_id = user_ids[idx]\n",
    "                item_id = item_ids[idx]\n",
    "                label = labels[idx][0]\n",
    "                rating = labels[idx][1]\n",
    "                rated_items = user_rated_items[user_id].copy()\n",
    "                user_description = self.get_user_description(rated_items, item_id)\n",
    "                batch_user_descriptions.append(user_description)\n",
    "                batch_num_items.append(user_description.__len__())\n",
    "                batch_user_ids.append(user_id)\n",
    "                batch_item_ids.append(item_id)\n",
    "                batch_labels.append(label)\n",
    "                batch_ratings.append(rating)\n",
    "            max_user_des = max(batch_num_items)\n",
    "            batch_user_descriptions = self.padding_user_description(batch_user_descriptions, mask, max_user_des)\n",
    "            batch_data.append((np.array(batch_user_descriptions, dtype=np.int32),\n",
    "                               np.array(batch_user_ids, dtype=np.int32),\n",
    "                               np.array(batch_item_ids, dtype=np.int32),\n",
    "                               np.array(batch_num_items, dtype=np.float32),\n",
    "                               np.array(batch_labels, dtype=np.float32),\n",
    "                               np.array(batch_ratings, dtype=np.float32)\n",
    "                               ))\n",
    "#             if i % 1000 == 0:\n",
    "#                 print(f'done %d/%d iter' % (i, num_iter))\n",
    "#             if i > 1000:\n",
    "#                 break\n",
    "        return batch_data, num_iter\n",
    "\n",
    "    def get_user_description(self, rated_items, item_id):\n",
    "        for i in range(rated_items.__len__()):\n",
    "            if rated_items[i] == item_id:\n",
    "                rated_items[i] = rated_items[-1]\n",
    "                del rated_items[-1]\n",
    "                break\n",
    "        return rated_items\n",
    "\n",
    "    def padding_user_description(self, batch_user_descriptions, mask, max_len):\n",
    "        for i in range(batch_user_descriptions.__len__()):\n",
    "            batch_user_descriptions[i] = batch_user_descriptions[i] + [mask] * (max_len - batch_user_descriptions[i].__len__())\n",
    "        return batch_user_descriptions\n",
    "\n",
    "    def get_batch(self):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FISM(Model):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super(FISM, self).__init__()\n",
    "        self.embedding_size = args['embedding_size']\n",
    "        self.alpha = args['alpha']\n",
    "        self.beta = args['beta']\n",
    "        self.gamma = args['gamma']\n",
    "        self.lambda_ = args['lambda_']\n",
    "        self.verbose = args['verborse']\n",
    "        self.num_items = args['num_items']\n",
    "        self.num_users = args['num_users']\n",
    "        self.confidence_factor = args['confidence_factor']\n",
    "        self.Q_norms = None\n",
    "        self.P_norms = None\n",
    "        self.item_norms = None\n",
    "        self.item_vectors = None\n",
    "        # self.P = tf.keras.layers.Embedding(input_dim=self.num_items + 1, output_dim=self.embedding_size,\n",
    "        #                                    embeddings_initializer=TruncatedNormal(mean=0., stddev=0.1), trainable=True)\n",
    "\n",
    "        # p_var = tf.Variable(tf.keras.initializers.TruncatedNormal(mean=0, stddev=0.1)(shape=[self.num_items, self.embedding_size]))\n",
    "        # mask_value = tf.constant(0, shape=(1, self.embedding_size), dtype=tf.float32)\n",
    "        # p_var = tf.concat((p_var, mask_value), axis=0)\n",
    "        # self.P = tf.keras.layers.Embedding(input_dim=23607, output_dim=50, trainable=True, weights=[p_var.numpy()])\n",
    "\n",
    "        self.P = tf.Variable(\n",
    "            tf.random.truncated_normal(shape=[self.num_items, self.embedding_size], mean=0, stddev=0.1))\n",
    "        self.mask_value = tf.constant(0, shape=(1, self.embedding_size), dtype=tf.float32)\n",
    "        self.Q = tf.Variable(\n",
    "            tf.random.truncated_normal(shape=[self.num_items, self.embedding_size], mean=0, stddev=0.1))\n",
    "        self.bias_u = tf.keras.layers.Embedding(input_dim=self.num_users, output_dim=1,\n",
    "                                                embeddings_initializer=TruncatedNormal(mean=0., stddev=0.1))\n",
    "        self.bias_i = tf.keras.layers.Embedding(input_dim=self.num_items, output_dim=1,\n",
    "                                                embeddings_initializer=TruncatedNormal(mean=0., stddev=0.1))\n",
    "        #         self.optimizer = tf.optimizers.Adagrad(learning_rate=self.learning_rate, initial_accumulator_value=1)\n",
    "        # self.optimizer = tf.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "\n",
    "    def call(self, user_descriptions, user_ids, item_ids, num_items):\n",
    "        user_bias = self.bias_u(user_ids)\n",
    "        item_bias = self.bias_i(item_ids)\n",
    "        # user_rated_items_embedding = self.P(user_descriptions)\n",
    "        P_with_mask = tf.concat([self.P, self.mask_value], axis=0)\n",
    "        user_rated_items_embedding = tf.nn.embedding_lookup(P_with_mask, user_descriptions)\n",
    "        # items_embedding = self.Q(item_ids)\n",
    "        items_embedding = tf.nn.embedding_lookup(self.Q, item_ids)\n",
    "        user_des = tf.reduce_sum(user_rated_items_embedding, axis=1)\n",
    "        coefficient = tf.pow(num_items, -tf.constant(self.alpha, dtype=tf.float32))\n",
    "        r = tf.squeeze(user_bias) + tf.squeeze(item_bias) + tf.math.multiply(coefficient, tf.reduce_sum(\n",
    "            tf.math.multiply(user_des, items_embedding), axis=1))\n",
    "        return r\n",
    "\n",
    "    def loss_fn_old(self, predictions, labels, ratings):\n",
    "        confidences = 1 + self.confidence_factor * ratings\n",
    "        loss = tf.reduce_sum(tf.math.multiply(confidences, tf.math.square(predictions - labels)))\n",
    "        loss += self.beta * (tf.reduce_sum(tf.math.square(self.P)) + tf.reduce_sum(\n",
    "            tf.math.square(self.Q)))\n",
    "        loss += self.lambda_ * tf.reduce_sum(tf.math.square(self.bias_u.embeddings)) + self.gamma * tf.reduce_sum(\n",
    "            tf.math.square(self.bias_i.embeddings))\n",
    "        return loss\n",
    "\n",
    "    def loss_fn(self, predictions, labels, ratings):\n",
    "        predictions = tf.math.sigmoid(predictions)\n",
    "        #         predictions = tf.cast(predictions, tf.float64)\n",
    "        predictions = tf.clip_by_value(predictions, clip_value_min=1e-7, clip_value_max=1 - 1e-7)\n",
    "        cross_entropy_elements = -(tf.math.multiply(labels, tf.math.log(predictions)) +\n",
    "                                   tf.math.multiply(1 - labels, tf.math.log(1 - predictions)))\n",
    "        confidences = 1 + self.confidence_factor * ratings\n",
    "        loss = tf.reduce_sum(tf.math.multiply(confidences, cross_entropy_elements))\n",
    "        loss += self.beta * (tf.reduce_sum(tf.math.square(self.P)) + tf.reduce_sum(tf.math.square(self.Q)))\n",
    "        loss += self.lambda_ * tf.reduce_sum(tf.math.square(self.bias_u.embeddings)) + self.gamma * tf.reduce_sum(\n",
    "            tf.math.square(self.bias_i.embeddings))\n",
    "        return loss\n",
    "\n",
    "    def prepare_for_prediction(self):\n",
    "        self.Q_norms = tf.sqrt(tf.reduce_sum(tf.square(self.Q), axis=1))\n",
    "        self.P_norms = tf.sqrt(tf.reduce_sum(tf.square(self.P), axis=1))\n",
    "        self.item_vectors = tf.concat([self.P, self.Q], axis=1)\n",
    "        self.item_norms = tf.sqrt(tf.reduce_sum(tf.square(self.item_vectors), axis=1))\n",
    "\n",
    "    def sim_items(self, item_id, top_n: int = 100):\n",
    "        item_embedded = tf.nn.embedding_lookup(self.P, item_id)\n",
    "        item_embedded = tf.reshape(item_embedded, shape=(self.embedding_size, -1))\n",
    "        scores = tf.matmul(self.Q, item_embedded)\n",
    "        scores = tf.squeeze(scores)\n",
    "        scores = scores / (self.Q_norms * self.P_norms[item_id])\n",
    "        scores = scores.numpy()\n",
    "        best = np.argpartition(scores, -top_n)[-top_n:]\n",
    "        return sorted(zip(best, scores[best]), key=lambda x: -x[1])\n",
    "\n",
    "    def sim_items_concat_pq(self, item_id, top_n: int = 100):\n",
    "        item_embedded = tf.nn.embedding_lookup(self.item_vectors, item_id)\n",
    "        item_embedded = tf.reshape(item_embedded, shape=(2 * self.embedding_size, -1))\n",
    "        scores = tf.matmul(self.item_vectors, item_embedded)\n",
    "        scores = tf.squeeze(scores)\n",
    "        scores = scores / (self.item_norms * self.item_norms[item_id])\n",
    "        scores = scores.numpy()\n",
    "        best = np.argpartition(scores, -top_n)[-top_n:]\n",
    "        return sorted(zip(best, scores[best]), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def predict_top_n(model, user_id, user_rated_items, top_n=100, batch_size=512):\n",
    "    rated_items = set(user_rated_items[user_id])\n",
    "    predicts = []\n",
    "    user_descriptions = []\n",
    "    user_ids = []\n",
    "    item_ids = []\n",
    "    num_items = []\n",
    "    for item_id in range(model.num_items):\n",
    "        if rated_items.__contains__(item_id):\n",
    "            user_descriptions.append(list(rated_items.difference([item_id])) + [model.num_items])\n",
    "            user_ids.append(user_id)\n",
    "            item_ids.append(item_id)\n",
    "            num_items.append(rated_items.__len__() - 1)\n",
    "        else:\n",
    "            user_descriptions.append(list(rated_items.difference([item_id])))\n",
    "            user_ids.append(user_id)\n",
    "            item_ids.append(item_id)\n",
    "            num_items.append(rated_items.__len__())\n",
    "        if user_descriptions.__len__() >= batch_size:\n",
    "            batch_predict = model(np.array(user_descriptions, dtype=np.int32),\n",
    "                                  np.array(user_ids, dtype=np.int32),\n",
    "                                  np.array(item_ids, dtype=np.int32),\n",
    "                                  np.array(num_items, dtype=np.float32))\n",
    "            predicts += list(batch_predict.numpy())\n",
    "            user_descriptions = []\n",
    "            user_ids = []\n",
    "            item_ids = []\n",
    "            num_items = []\n",
    "    batch_predict = model(np.array(user_descriptions, dtype=np.int32),\n",
    "                          np.array(user_ids, dtype=np.int32),\n",
    "                          np.array(item_ids, dtype=np.int32),\n",
    "                          np.array(num_items, dtype=np.float32))\n",
    "    predicts += list(batch_predict.numpy())\n",
    "    items_score = [(iid, score) for iid, score in enumerate(predicts)]\n",
    "    items_score.sort(key=lambda x: x[1], reverse=True)\n",
    "    return items_score[:top_n]\n",
    "\n",
    "\n",
    "def hit_rate_evaluate(fism_model, user_rated_items):\n",
    "    total_items = 0\n",
    "    in_train_count = 0\n",
    "    count = 0\n",
    "    count_hit = 0\n",
    "    for test_user in tqdm(test_users_in_train):\n",
    "        user_session_id = test_user[0]\n",
    "        if user_idx_dict.__contains__(user_session_id):\n",
    "            user_id = user_idx_dict[user_session_id]\n",
    "            rated_items = test_user[1]\n",
    "            rec_top_n = predict_top_n(fism_model, user_id, user_rated_items, batch_size=4096, top_n=100)\n",
    "            top_item_ids = {rec_item[0] for rec_item in rec_top_n}\n",
    "            for rated_item in rated_items:\n",
    "                if item_idx_dict.__contains__(rated_item):\n",
    "                    item_id = item_idx_dict[rated_item]\n",
    "                    in_train_count += 1\n",
    "                    if top_item_ids.__contains__(item_id):\n",
    "                        count_hit += 1\n",
    "        total_items += rated_items.__len__()\n",
    "        count += 1\n",
    "        if count > 300:\n",
    "            break\n",
    "    in_train_rate = in_train_count / total_items\n",
    "    hit_rate = count_hit / total_items\n",
    "    return in_train_rate, hit_rate\n",
    "\n",
    "\n",
    "def rank_score_evaluate(fism_model, user_rated_items):\n",
    "    count = 0\n",
    "    list_user_ranks = []\n",
    "    num_item = item_idx_dict.__len__()\n",
    "    total_pred = 0\n",
    "    pred_hit = 0\n",
    "    for user_session_id, list_cluster_ids in tqdm(test_users_in_train):\n",
    "        user_id = user_idx_dict[user_session_id]\n",
    "        list_rec_items = predict_top_n(fism_model, user_id, user_rated_items, batch_size=4096, top_n=-1)\n",
    "        rec_items_idx = {item_id: idx + 1 for idx, (item_id, score) in enumerate(list_rec_items)}\n",
    "        user_ranks = []\n",
    "        for cluster_id in list_cluster_ids:\n",
    "            total_pred += 1\n",
    "            if item_idx_dict.__contains__(cluster_id):\n",
    "                pred_hit += 1\n",
    "                if rec_items_idx.__len__() > 0:\n",
    "                    item_idx = item_idx_dict[cluster_id]\n",
    "                    if rec_items_idx.__contains__(item_idx):\n",
    "                        pred_rank = rec_items_idx[item_idx] / num_item\n",
    "                        user_ranks.append(pred_rank)\n",
    "        list_user_ranks.append(user_ranks)\n",
    "\n",
    "        count += 1\n",
    "        if count > 300:\n",
    "            break\n",
    "    rank_mean_users = []\n",
    "    for user_ranks in list_user_ranks:\n",
    "        if user_ranks.__len__() > 0:\n",
    "            rank_mean_users.append(np.mean(user_ranks))\n",
    "    return np.mean(rank_mean_users), pred_hit / total_pred\n",
    "\n",
    "\n",
    "def item_sim_rec_top_n(fism_model, user_cluster_ids=[], top_n=100):\n",
    "    dict_max_score = {}\n",
    "    for his_cluster_id in user_cluster_ids:\n",
    "        if item_idx_dict.__contains__(his_cluster_id):\n",
    "            item_idx = item_idx_dict[his_cluster_id]\n",
    "            rec_items = fism_model.sim_items_concat_pq(item_id=item_idx, top_n=top_n)\n",
    "            for item_idx, score in rec_items:\n",
    "                if not dict_max_score.__contains__(item_idx):\n",
    "                    dict_max_score[item_idx] = []\n",
    "                dict_max_score[item_idx].append(score)\n",
    "    for item_idx, list_scores in dict_max_score.items():\n",
    "        dict_max_score[item_idx] = max(list_scores)\n",
    "    list_rec_items = list(dict_max_score.items())\n",
    "    list_rec_items.sort(key=lambda x: x[1], reverse=True)\n",
    "    return list_rec_items[:top_n]\n",
    "\n",
    "\n",
    "def item_sim_hit_rate_evaluate(fism_model, top_n=100, num_his=3):\n",
    "    need_pred = 0\n",
    "    in_train_set = 0\n",
    "\n",
    "    list_user_hits = []\n",
    "    count = 0\n",
    "    for uid, list_cluster_ids in tqdm(test_users):\n",
    "        random_split = random.randint(num_his, list_cluster_ids.__len__() - 1)\n",
    "        history_cluster_ids = list_cluster_ids[random_split - num_his:random_split]\n",
    "        predict_cluster_ids = list_cluster_ids[random_split: random_split + 10]\n",
    "        top_n_rec = item_sim_rec_top_n(fism_model, user_cluster_ids=history_cluster_ids, top_n=top_n)\n",
    "        rec_items_idx = {item_id: idx + 1 for idx, (item_id, distance) in enumerate(top_n_rec)}\n",
    "        #     print(top_n_rec)\n",
    "        user_hit = 0\n",
    "        for pred_cluster_id in predict_cluster_ids:\n",
    "            need_pred += 1\n",
    "            if item_idx_dict.__contains__(pred_cluster_id):\n",
    "                in_train_set += 1\n",
    "                if rec_items_idx.__contains__(item_idx_dict[pred_cluster_id]):\n",
    "                    user_hit += 1\n",
    "        list_user_hits.append((user_hit, predict_cluster_ids.__len__()))\n",
    "        count += 1\n",
    "        if count > 3000:\n",
    "            break\n",
    "    hit_sum = sum([hit for hit, pred_len in list_user_hits])\n",
    "    pred_sum = sum([pred_len for hit, pred_len in list_user_hits])\n",
    "    hit_rate = hit_sum / pred_sum\n",
    "    in_train_set_rate = in_train_set / need_pred\n",
    "    return need_pred, hit_rate, in_train_set_rate\n",
    "\n",
    "\n",
    "def item_sim_rank_score(fism_model):\n",
    "    count = 0\n",
    "    list_user_ranks = []\n",
    "    num_item = item_idx_dict.__len__()\n",
    "    total_pred = 0\n",
    "    pred_hit = 0\n",
    "    for uid, list_cluster_ids in tqdm(test_users):\n",
    "        random_split = random.randint(3, list_cluster_ids.__len__() - 1)\n",
    "        history_cluster_ids = list_cluster_ids[random_split - 3:random_split]\n",
    "        pred_cluster_ids = list_cluster_ids[random_split: random_split + 10]\n",
    "        dict_max_score = {}\n",
    "        for his_cluster_id in history_cluster_ids:\n",
    "            if item_idx_dict.__contains__(his_cluster_id):\n",
    "                item_idx = item_idx_dict[his_cluster_id]\n",
    "                rec_items = fism_model.sim_items_concat_pq(item_id=item_idx, top_n=num_item)\n",
    "                for item_idx, score in rec_items:\n",
    "                    if not dict_max_score.__contains__(item_idx):\n",
    "                        dict_max_score[item_idx] = []\n",
    "                    dict_max_score[item_idx].append(score)\n",
    "        for item_idx, list_scores in dict_max_score.items():\n",
    "            dict_max_score[item_idx] = max(list_scores)\n",
    "\n",
    "        list_rec_items = list(dict_max_score.items())\n",
    "        list_rec_items.sort(key=lambda x: x[1], reverse=True)\n",
    "        rec_items_idx = {item_id: idx + 1 for idx, (item_id, score) in enumerate(list_rec_items)}\n",
    "\n",
    "        user_ranks = []\n",
    "        for pred_cluster_id in pred_cluster_ids:\n",
    "            total_pred += 1\n",
    "            if item_idx_dict.__contains__(pred_cluster_id):\n",
    "                pred_hit += 1\n",
    "                if rec_items_idx.__len__() > 0:\n",
    "                    item_idx = item_idx_dict[pred_cluster_id]\n",
    "                    pred_rank = rec_items_idx[item_idx] / num_item\n",
    "                    user_ranks.append(pred_rank)\n",
    "        list_user_ranks.append(user_ranks)\n",
    "\n",
    "        count += 1\n",
    "        if count > 500:\n",
    "            break\n",
    "    rank_mean_users = []\n",
    "    for user_ranks in list_user_ranks:\n",
    "        if user_ranks.__len__() > 0:\n",
    "            rank_mean_users.append(np.mean(user_ranks))\n",
    "    return np.mean(rank_mean_users), pred_hit / total_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = 'F:\\Projects\\ListingRecommendation\\data\\oto.com.vn\\\\ncf\\\\'\n",
    "\n",
    "auto_id_cluster_dict = pickle.load(open(base_folder + 'auto_id_cluster_dict.pkl', 'rb'))\n",
    "cluster_auto_ids_dict = pickle.load(open(base_folder + 'cluster_auto_ids_dict.pkl', 'rb'))\n",
    "idx_cluster_dict = pickle.load(open(base_folder + 'idx_cluster_dict.pkl', 'rb'))\n",
    "idx_user_dict = pickle.load(open(base_folder + 'idx_user_dict.pkl', 'rb'))\n",
    "idx_item_dict = pickle.load(open(base_folder + 'idx_item_dict.pkl', 'rb'))\n",
    "test_users = pickle.load(open(base_folder + 'test_users.pkl', 'rb'))\n",
    "\n",
    "user_idx_dict = {user_id: idx for idx, user_id in idx_user_dict.items()}\n",
    "item_idx_dict = {item_id: idx for idx, item_id in idx_item_dict.items()}\n",
    "\n",
    "test_users_in_train = []\n",
    "for user_session_id, items in test_users:\n",
    "    if user_idx_dict.__contains__(user_session_id):\n",
    "        test_users_in_train.append((user_session_id, items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model, optimizer, user_descriptions, user_ids, item_ids, num_items, labels, ratings):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(user_descriptions, user_ids, item_ids, num_items)\n",
    "        loss = model.loss_fn(predictions, labels, ratings)\n",
    "    gradients = tape.gradient(target=loss, sources=model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def training(fism_model, optimizer, dataset, num_epochs, pretrained=False):\n",
    "    start_epoch = tf.Variable(0, dtype=tf.int32)\n",
    "    ckpt = tf.train.Checkpoint(fism_model=fism_model, start_epoch=start_epoch)\n",
    "    manager = tf.train.CheckpointManager(checkpoint=ckpt, directory='./fism_ckpt', max_to_keep=3)\n",
    "    if pretrained:\n",
    "        ckpt.restore(manager.latest_checkpoint)\n",
    "    rated_data = dataset.get_rated_data()\n",
    "    user_rated_items = dict()\n",
    "    for user_id, item_id, rate in rated_data:\n",
    "        if not user_rated_items.__contains__(user_id):\n",
    "            user_rated_items[user_id] = []\n",
    "        user_rated_items[user_id].append(item_id)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = tf.constant(0, tf.float32)\n",
    "        # print('start training epoch: ' + str(epoch))\n",
    "        start_load_data = time()\n",
    "        all_batch_data, num_iter = dataset.generate_train_data()\n",
    "        load_data_time = time() - start_load_data\n",
    "        start_train_time = time()\n",
    "        for user_descriptions, user_ids, item_ids, num_items, labels, ratings in tqdm(all_batch_data):\n",
    "            loss_step = train_step(fism_model, optimizer, user_descriptions, user_ids, item_ids, num_items, labels, ratings)\n",
    "            #             print('loss_step: ', loss_step.numpy())\n",
    "            train_loss += loss_step\n",
    "        train_time = time() - start_train_time\n",
    "        print('epoch: ', epoch, '. load data time: ', load_data_time, '. train time: ', train_time ,'. train loss: ', train_loss.numpy() / (all_batch_data.__len__()))\n",
    "        if epoch % 2 == 0:\n",
    "            fism_model.prepare_for_prediction()\n",
    "            need_pred, item_sim_hit_rate, in_train_set_rate = item_sim_hit_rate_evaluate(fism_model)\n",
    "            item_sim_rank, rank_in_train_set_rate = item_sim_rank_score(fism_model)\n",
    "\n",
    "            ### user\n",
    "            in_train_rate, user_hit_rate = hit_rate_evaluate(fism_model, user_rated_items)\n",
    "            user_rank_score, rank_in_train_set = rank_score_evaluate(fism_model, user_rated_items)\n",
    "\n",
    "            score = {'need_pred': need_pred,\n",
    "                     'item_sim_hit_rate': item_sim_hit_rate,\n",
    "                     'in_train_set_rate': in_train_set_rate,\n",
    "                     'item_sim_rank': item_sim_rank,\n",
    "\n",
    "\n",
    "                     'cf_hit_rate': user_hit_rate,\n",
    "                     'cf_in_train_set_rate': in_train_rate,\n",
    "                     'cf_rank': user_rank_score}\n",
    "\n",
    "            print('epoch: {}, score: {}'.format(epoch, score))\n",
    "            # start_epoch = tf.Variable(start_epoch.numpy() + epoch + 1, dtype=tf.int32)\n",
    "            ckpt.start_epoch.assign_add(epoch + 1)\n",
    "            manager.save()\n",
    "            print('done save at epoch: ', start_epoch.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataSet(base_folder + 'train.csv', base_folder + 'test.csv', negative_sample=3, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dict()\n",
    "args['embedding_size'] = 50\n",
    "args['alpha'] = 0.6\n",
    "args['beta'] = 0.0005\n",
    "# args['gamma'] = 0.001\n",
    "# args['lambda_'] = 0.001\n",
    "args['gamma'] = 0.000\n",
    "args['lambda_'] = 0.000\n",
    "args['verborse'] = 1\n",
    "args['num_items'] = dataset.num_items\n",
    "args['num_users'] = dataset.num_users\n",
    "args['confidence_factor'] = 1\n",
    "\n",
    "fism = FISM(args)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\administrator\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:31: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1076.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\administrator\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0827 10:13:30.513999  8260 def_function.py:598] 5 out of the last 8 calls to <function train_step at 0x0000000055D7A9D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "W0827 10:13:31.013028  8260 def_function.py:598] 5 out of the last 11 calls to <function train_step at 0x0000000055D7A9D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "W0827 10:13:31.436052  8260 def_function.py:598] 5 out of the last 12 calls to <function train_step at 0x0000000055D7A9D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch:  0 . load data time:  11.828676700592041 . train time:  53.63206744194031 . train loss:  700.7676579925651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\administrator\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:122: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1369.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\administrator\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:153: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1369.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\administrator\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:45: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=320.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\administrator\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:73: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=320.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 0, score: {'need_pred': 3057, 'item_sim_hit_rate': 0.27641478573765127, 'in_train_set_rate': 0.7595682041216879, 'item_sim_rank': 0.13751833120494375, 'cf_hit_rate': 0.1968390804597701, 'cf_in_train_set_rate': 0.7825670498084292, 'cf_rank': 0.10465055133530193}\n",
      "done save at epoch:  1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1076.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch:  1 . load data time:  14.030802726745605 . train time:  47.53571891784668 . train loss:  427.82992565055764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1076.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch:  2 . load data time:  11.976684808731079 . train time:  47.65972590446472 . train loss:  343.689968633829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1369.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1369.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=320.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=320.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 2, score: {'need_pred': 3006, 'item_sim_hit_rate': 0.30538922155688625, 'in_train_set_rate': 0.7717897538256819, 'item_sim_rank': 0.10465495917765187, 'cf_hit_rate': 0.23515325670498086, 'cf_in_train_set_rate': 0.7825670498084292, 'cf_rank': 0.08992122158762109}\n",
      "done save at epoch:  4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1076.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch:  3 . load data time:  12.139694213867188 . train time:  47.46071481704712 . train loss:  312.93892309479554\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1076.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch:  4 . load data time:  14.646837711334229 . train time:  49.38682460784912 . train loss:  297.0112976301115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1369.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1369.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=320.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=320.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 4, score: {'need_pred': 3025, 'item_sim_hit_rate': 0.2955371900826446, 'in_train_set_rate': 0.7649586776859504, 'item_sim_rank': 0.1161446226447485, 'cf_hit_rate': 0.26580459770114945, 'cf_in_train_set_rate': 0.7825670498084292, 'cf_rank': 0.09863025606742123}\n",
      "done save at epoch:  9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1076.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch:  5 . load data time:  12.103692054748535 . train time:  47.321706771850586 . train loss:  288.7306284851301\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1076.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch:  6 . load data time:  11.744671821594238 . train time:  50.21387219429016 . train loss:  283.55248025092936\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1369.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1369.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=320.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=320.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 6, score: {'need_pred': 2983, 'item_sim_hit_rate': 0.32215890043580286, 'in_train_set_rate': 0.7703654039557493, 'item_sim_rank': 0.11138210306473398, 'cf_hit_rate': 0.27155172413793105, 'cf_in_train_set_rate': 0.7825670498084292, 'cf_rank': 0.09987771134145011}\n",
      "done save at epoch:  16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1076.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch:  7 . load data time:  11.75167202949524 . train time:  48.345765113830566 . train loss:  279.6188719795539\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1076.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch:  8 . load data time:  14.06880497932434 . train time:  45.29659080505371 . train loss:  275.76577021375465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1369.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1369.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=320.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=320.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 8, score: {'need_pred': 3030, 'item_sim_hit_rate': 0.30066006600660067, 'in_train_set_rate': 0.76006600660066, 'item_sim_rank': 0.12160092634440826, 'cf_hit_rate': 0.2825670498084291, 'cf_in_train_set_rate': 0.7825670498084292, 'cf_rank': 0.10360479386649928}\n",
      "done save at epoch:  25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1076.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch:  9 . load data time:  12.893737316131592 . train time:  46.02763271331787 . train loss:  273.10484433085503\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1076.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch:  10 . load data time:  11.766672849655151 . train time:  45.10357999801636 . train loss:  269.92718982342006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1369.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1369.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=320.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=320.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 10, score: {'need_pred': 3019, 'item_sim_hit_rate': 0.31069890692282215, 'in_train_set_rate': 0.7687976151043392, 'item_sim_rank': 0.12502236170477296, 'cf_hit_rate': 0.2792145593869732, 'cf_in_train_set_rate': 0.7825670498084292, 'cf_rank': 0.1028874701652126}\n",
      "done save at epoch:  36\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1076.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch:  11 . load data time:  13.033745527267456 . train time:  50.30887722969055 . train loss:  267.9796119888476\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1076.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch:  12 . load data time:  12.696726322174072 . train time:  48.23975896835327 . train loss:  266.14155436802974\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1369.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1369.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=320.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=320.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 12, score: {'need_pred': 3085, 'item_sim_hit_rate': 0.2985413290113452, 'in_train_set_rate': 0.7614262560777958, 'item_sim_rank': 0.11920139164111163, 'cf_hit_rate': 0.29118773946360155, 'cf_in_train_set_rate': 0.7825670498084292, 'cf_rank': 0.10865541398109826}\n",
      "done save at epoch:  49\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1076.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch:  13 . load data time:  13.400766611099243 . train time:  48.56577777862549 . train loss:  265.0007841542751\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1076.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch:  14 . load data time:  11.862678289413452 . train time:  47.7607319355011 . train loss:  263.2503775557621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1369.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1369.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=320.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=320.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 14, score: {'need_pred': 3089, 'item_sim_hit_rate': 0.3072191647782454, 'in_train_set_rate': 0.7730657170605374, 'item_sim_rank': 0.12534601729691683, 'cf_hit_rate': 0.2792145593869732, 'cf_in_train_set_rate': 0.7825670498084292, 'cf_rank': 0.10860631937568954}\n",
      "done save at epoch:  64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1076.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch:  15 . load data time:  14.054803609848022 . train time:  54.60412335395813 . train loss:  261.4638998605948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1076.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch:  16 . load data time:  13.87579345703125 . train time:  56.855252265930176 . train loss:  260.08460153345726\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1369.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1369.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-ceda0974713f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfism\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-7a1a85d80d73>\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(fism_model, optimizer, dataset, num_epochs, pretrained)\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0mfism_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_for_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mneed_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem_sim_hit_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0min_train_set_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem_sim_hit_rate_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfism_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0mitem_sim_rank\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrank_in_train_set_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem_sim_rank_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfism_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[1;31m### user\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-a142d5ca2fb0>\u001b[0m in \u001b[0;36mitem_sim_rank_score\u001b[1;34m(fism_model)\u001b[0m\n\u001b[0;32m    163\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdict_max_score\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__contains__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m                         \u001b[0mdict_max_score\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m                     \u001b[0mdict_max_score\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mitem_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist_scores\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdict_max_score\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mdict_max_score\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "training(fism, optimizer, dataset, num_epochs=100, pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bpr_mf score:\n",
    "    {'need_pred': 2264, 'hit_rate': 0.28931095406360424, 'in_train_set_rate': 0.7628091872791519, 'rank': 0.11048865212027534, 'cf_need_pred': 2221, 'cf_hit_rate': 0.14633048176497074, 'cf_in_train_set_rate': 0.7757766771724448, 'cf_rank': 0.12322572834338055"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rated_data = dataset.get_rated_data()\n",
    "user_rated_items = dict()\n",
    "for user_id, item_id, rate in rated_data:\n",
    "    if not user_rated_items.__contains__(user_id):\n",
    "        user_rated_items[user_id] = []\n",
    "    user_rated_items[user_id].append(item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\administrator\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:1: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(10)):\n",
    "    predict_top_n(fism, i, user_rated_items, batch_size=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23606"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fism.num_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9999898700090192, 0.9999498278353162, 0.9820137900379085)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(11.5), sigmoid(9.9), sigmoid(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3685, 12.186567),\n",
       " (13, 10.799454),\n",
       " (13168, 9.855587),\n",
       " (12801, 9.417419),\n",
       " (12855, 9.372033),\n",
       " (14, 9.280977),\n",
       " (6248, 9.12025),\n",
       " (1686, 8.478687),\n",
       " (14536, 8.3661785),\n",
       " (1773, 8.123233),\n",
       " (5509, 8.041772),\n",
       " (2496, 7.95879),\n",
       " (2603, 7.692417),\n",
       " (6505, 7.6681337),\n",
       " (1008, 7.4739704),\n",
       " (17838, 7.378082),\n",
       " (15, 7.3428006),\n",
       " (17566, 7.290766),\n",
       " (5706, 7.2192183),\n",
       " (13345, 7.164782),\n",
       " (17894, 7.150196),\n",
       " (6199, 7.13529),\n",
       " (4730, 7.1103134),\n",
       " (16, 7.0262804),\n",
       " (12, 6.917858),\n",
       " (18, 6.881666),\n",
       " (11161, 6.765689),\n",
       " (13875, 6.6233025),\n",
       " (1687, 6.592203),\n",
       " (21115, 6.575879),\n",
       " (17314, 6.4467955),\n",
       " (8981, 6.3982387),\n",
       " (1650, 6.23897),\n",
       " (18866, 6.2379932),\n",
       " (9207, 6.07539),\n",
       " (13746, 6.067525),\n",
       " (8420, 6.053816),\n",
       " (21997, 6.026434),\n",
       " (12826, 5.9899263),\n",
       " (13438, 5.9805274),\n",
       " (17696, 5.7239513),\n",
       " (5864, 5.7172956),\n",
       " (488, 5.7093563),\n",
       " (3275, 5.6911182),\n",
       " (12416, 5.6675043),\n",
       " (21240, 5.6552343),\n",
       " (2577, 5.503698),\n",
       " (1369, 5.488298),\n",
       " (2464, 5.480237),\n",
       " (6398, 5.4657464),\n",
       " (5510, 5.437488),\n",
       " (6587, 5.3917327),\n",
       " (2787, 5.387827),\n",
       " (4986, 5.3456273),\n",
       " (11023, 5.329528),\n",
       " (19370, 5.3232813),\n",
       " (964, 5.3037176),\n",
       " (3897, 5.2579145),\n",
       " (3560, 5.160589),\n",
       " (4940, 5.1333504),\n",
       " (18851, 5.09369),\n",
       " (5511, 5.0417423),\n",
       " (4939, 5.008577),\n",
       " (3880, 4.973464),\n",
       " (3225, 4.9550757),\n",
       " (5508, 4.941159),\n",
       " (9469, 4.922241),\n",
       " (17, 4.895215),\n",
       " (6044, 4.847323),\n",
       " (6980, 4.8039584),\n",
       " (17088, 4.703926),\n",
       " (14181, 4.656571),\n",
       " (19618, 4.6246634),\n",
       " (13264, 4.5535483),\n",
       " (22758, 4.5519166),\n",
       " (6043, 4.521497),\n",
       " (18136, 4.466521),\n",
       " (12897, 4.4638534),\n",
       " (1649, 4.4262),\n",
       " (13995, 4.393031),\n",
       " (21346, 4.380568),\n",
       " (3274, 4.343949),\n",
       " (6394, 4.331298),\n",
       " (6245, 4.300787),\n",
       " (6045, 4.280802),\n",
       " (1940, 4.2504187),\n",
       " (19537, 4.242597),\n",
       " (7952, 4.206088),\n",
       " (20552, 4.1794333),\n",
       " (5273, 4.1691656),\n",
       " (20834, 4.142523),\n",
       " (11597, 4.1149187),\n",
       " (16526, 4.068448),\n",
       " (16877, 4.054243),\n",
       " (9336, 4.0377345),\n",
       " (3859, 3.9824982),\n",
       " (6993, 3.981687),\n",
       " (8565, 3.9631047),\n",
       " (170, 3.9393835),\n",
       " (5272, 3.932197)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_top_n(fism, 2, user_rated_items, batch_size=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_u_matrix_df = pickle.load(open(base_folder + 'raw_u_matrix_df.pkl', 'rb'))\n",
    "u_matrix_df = pickle.load(open(base_folder + 'u_matrix_df.pkl', 'rb'))\n",
    "auto_id_cluster_dict = pickle.load(open(base_folder + 'auto_id_cluster_dict.pkl', 'rb'))\n",
    "cluster_auto_ids_dict = pickle.load(open(base_folder + 'cluster_auto_ids_dict.pkl', 'rb'))\n",
    "idx_cluster_dict = pickle.load(open(base_folder + 'idx_cluster_dict.pkl', 'rb'))\n",
    "idx_user_dict = pickle.load(open(base_folder + 'idx_user_dict.pkl', 'rb'))\n",
    "idx_item_dict = pickle.load(open(base_folder + 'idx_item_dict.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_model = pickle.load(open('F:\\Projects\\ListingRecommendation\\data\\oto.com.vn' + os.sep + 'cluster_model.pkl', 'rb'))\n",
    "idx_year_dict = dict()\n",
    "for year_item in cluster_model['year_value_ids']:\n",
    "    value = ''\n",
    "    if year_item.__contains__('gte'):\n",
    "        value += (str(year_item['gte']) + '-')\n",
    "    if year_item.__contains__('lt'):\n",
    "        value += ('-' +str(year_item['lt']))\n",
    "    idx_year_dict[year_item['id']] = value\n",
    "\n",
    "idx_price_dict = dict()\n",
    "for price_item in cluster_model['price_value_ids']:\n",
    "    value = ''\n",
    "    if price_item.__contains__('gte'):\n",
    "        value += (str(price_item['gte']) + '-')\n",
    "    if price_item.__contains__('lt'):\n",
    "        value += ('-' +str(price_item['lt']))\n",
    "    idx_price_dict[price_item['id']] = value\n",
    "# idx_exterior_color_dict = {idx:color for color, idx in cluster_model['exterior_color_value_ids'].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = 'F:\\Projects\\ListingRecommendation\\data\\oto.com.vn\\\\ncf\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(Button(description='random test', style=ButtonStyle()), Output()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def random_test_items_rec():\n",
    "    clear_output()\n",
    "    random_user_idx = random.randint(0, fism.num_users)    \n",
    "    rec_items = predict_top_n(fism, random_user_idx, user_rated_items, batch_size=4096)\n",
    "    count_display = 0\n",
    "    for item_id in user_rated_items[random_user_idx][:50]:\n",
    "        cluster_key = idx_cluster_dict[idx_item_dict[item_id]]    \n",
    "        list_keys = list(cluster_key)\n",
    "        list_keys[2] = idx_year_dict[list_keys[2]]\n",
    "        list_keys[3] = idx_price_dict[list_keys[3]]\n",
    "#         list_keys[5] = idx_exterior_color_dict[list_keys[5]]\n",
    "        print(str(cluster_key) + '\\t----\\t' + str(list_keys))\n",
    "        count_display += 1\n",
    "        if count_display> 50:\n",
    "            break\n",
    "    count_display = 0\n",
    "    print('-' * 60)\n",
    "    print('-' * 60)\n",
    "    print('-' * 60)\n",
    "    for item_id, score in rec_items:\n",
    "        cluster_key = idx_cluster_dict[idx_item_dict[item_id]]    \n",
    "        list_keys = list(cluster_key)\n",
    "        list_keys[2] = idx_year_dict[list_keys[2]]\n",
    "        list_keys[3] = idx_price_dict[list_keys[3]]\n",
    "#         list_keys[5] = idx_exterior_color_dict[list_keys[5]]\n",
    "        print(str(cluster_key) + '\\t----\\t' + str(list_keys))\n",
    "        count_display += 1\n",
    "        if count_display> 20:\n",
    "            break\n",
    "\n",
    "button_other = widgets.Button(description='random test')\n",
    "out_other = widgets.Output()\n",
    "def on_button_clicked(_):\n",
    "      # \"linking function with output\"\n",
    "      with out_other:\n",
    "          # what happens when we press the button\n",
    "          random_test_items_rec()\n",
    "# linking button and function together using a button's method\n",
    "button_other.on_click(on_button_clicked)\n",
    "# displaying button and its output together\n",
    "widgets.VBox([button_other,out_other])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_users = pickle.load(open(base_folder + 'test_users.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1369"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_users.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_idx_dict = {user_id:idx for idx, user_id in idx_user_dict.items()}\n",
    "item_idx_dict = {item_id:idx for idx, item_id in idx_item_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_user = test_users[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = user_idx_dict[test_user[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2025, 1.5633965),\n",
       " (1803, 1.5580299),\n",
       " (19434, 1.5227872),\n",
       " (11960, 1.4938323),\n",
       " (19256, 1.410286),\n",
       " (13423, 1.3933122),\n",
       " (727, 1.3816149),\n",
       " (3343, 1.3751199),\n",
       " (16868, 1.3733063),\n",
       " (2314, 1.3621482),\n",
       " (4538, 1.3539121),\n",
       " (4610, 1.3500317),\n",
       " (13000, 1.3422687),\n",
       " (5623, 1.3413368),\n",
       " (11710, 1.3382826),\n",
       " (21291, 1.3306535),\n",
       " (4522, 1.3277571),\n",
       " (9044, 1.3231375),\n",
       " (6481, 1.3223011),\n",
       " (1292, 1.3207104),\n",
       " (21022, 1.311219),\n",
       " (867, 1.307471),\n",
       " (15436, 1.30653),\n",
       " (3369, 1.2994673),\n",
       " (17754, 1.2991552),\n",
       " (6203, 1.2987642),\n",
       " (2834, 1.295771),\n",
       " (5221, 1.2947232),\n",
       " (799, 1.2946851),\n",
       " (2535, 1.2936642),\n",
       " (17040, 1.2841288),\n",
       " (1491, 1.2779826),\n",
       " (14158, 1.2696521),\n",
       " (9145, 1.2670518),\n",
       " (6437, 1.2665544),\n",
       " (12965, 1.2644998),\n",
       " (2023, 1.2618456),\n",
       " (16546, 1.2596278),\n",
       " (22397, 1.2583109),\n",
       " (14248, 1.2575527),\n",
       " (19925, 1.2527559),\n",
       " (17842, 1.2497922),\n",
       " (10605, 1.2466345),\n",
       " (17641, 1.245878),\n",
       " (23262, 1.2438996),\n",
       " (11351, 1.242455),\n",
       " (325, 1.2423756),\n",
       " (2069, 1.2418395),\n",
       " (2214, 1.2403322),\n",
       " (2836, 1.2401394),\n",
       " (13126, 1.2387006),\n",
       " (16667, 1.2372184),\n",
       " (4326, 1.2321892),\n",
       " (15607, 1.2309735),\n",
       " (453, 1.2296147),\n",
       " (2876, 1.2291164),\n",
       " (1158, 1.2247895),\n",
       " (315, 1.223287),\n",
       " (12657, 1.2229259),\n",
       " (3711, 1.2198311),\n",
       " (11504, 1.2174873),\n",
       " (14780, 1.217346),\n",
       " (17582, 1.2172545),\n",
       " (289, 1.2134061),\n",
       " (2498, 1.2131879),\n",
       " (420, 1.2108523),\n",
       " (473, 1.2106602),\n",
       " (5689, 1.208127),\n",
       " (19958, 1.2076166),\n",
       " (2940, 1.2057558),\n",
       " (16121, 1.2042367),\n",
       " (7012, 1.2011395),\n",
       " (807, 1.1989113),\n",
       " (8763, 1.1987405),\n",
       " (17273, 1.196794),\n",
       " (2387, 1.196218),\n",
       " (1050, 1.19449),\n",
       " (16099, 1.1928804),\n",
       " (3633, 1.1921659),\n",
       " (3575, 1.192061),\n",
       " (2501, 1.1886616),\n",
       " (13593, 1.1876633),\n",
       " (6487, 1.1868818),\n",
       " (11548, 1.185218),\n",
       " (7746, 1.1849822),\n",
       " (9699, 1.183885),\n",
       " (16832, 1.1831644),\n",
       " (10850, 1.1821482),\n",
       " (6748, 1.1743441),\n",
       " (13439, 1.1740081),\n",
       " (1369, 1.1703292),\n",
       " (6299, 1.1699595),\n",
       " (16826, 1.1686223),\n",
       " (19414, 1.1682873),\n",
       " (1168, 1.1673709),\n",
       " (3856, 1.1665642),\n",
       " (13297, 1.1657027),\n",
       " (1092, 1.1649615),\n",
       " (4160, 1.1648393),\n",
       " (1073, 1.1645901),\n",
       " (11272, 1.1645694),\n",
       " (17489, 1.1637456),\n",
       " (17638, 1.1636021),\n",
       " (3255, 1.1634452),\n",
       " (13379, 1.16049),\n",
       " (19439, 1.1595759),\n",
       " (1268, 1.1594989),\n",
       " (22581, 1.1588123),\n",
       " (13591, 1.1584071),\n",
       " (6751, 1.158035),\n",
       " (291, 1.1575785),\n",
       " (7681, 1.157325),\n",
       " (3509, 1.1571114),\n",
       " (18108, 1.1565113),\n",
       " (7325, 1.1563003),\n",
       " (6528, 1.155515),\n",
       " (19597, 1.1550838),\n",
       " (21324, 1.1548228),\n",
       " (22027, 1.1540282),\n",
       " (12658, 1.1537724),\n",
       " (14320, 1.1524744),\n",
       " (6932, 1.1517351),\n",
       " (1040, 1.1514916),\n",
       " (1019, 1.1511015),\n",
       " (7301, 1.1510253),\n",
       " (22740, 1.1505177),\n",
       " (26, 1.1490624),\n",
       " (12277, 1.1484556),\n",
       " (7343, 1.1461723),\n",
       " (14563, 1.1460904),\n",
       " (2540, 1.1450902),\n",
       " (14649, 1.1439553),\n",
       " (3176, 1.1416249),\n",
       " (5785, 1.1415894),\n",
       " (5793, 1.1412411),\n",
       " (623, 1.1405177),\n",
       " (3760, 1.1404142),\n",
       " (19253, 1.1393037),\n",
       " (16232, 1.1391026),\n",
       " (17168, 1.1363618),\n",
       " (18027, 1.1361114),\n",
       " (21468, 1.1360916),\n",
       " (1659, 1.1352842),\n",
       " (2811, 1.1345987),\n",
       " (21193, 1.1333855),\n",
       " (447, 1.1331037),\n",
       " (7465, 1.1317108),\n",
       " (6054, 1.1309266),\n",
       " (17030, 1.1286027),\n",
       " (3668, 1.1279699),\n",
       " (7116, 1.1271697),\n",
       " (14399, 1.1263387),\n",
       " (1526, 1.1259928),\n",
       " (1842, 1.1255476),\n",
       " (19880, 1.1252228),\n",
       " (13502, 1.12517),\n",
       " (2220, 1.1248405),\n",
       " (956, 1.1240065),\n",
       " (1161, 1.1238109),\n",
       " (13540, 1.1232024),\n",
       " (16403, 1.1231606),\n",
       " (22151, 1.1229612),\n",
       " (1078, 1.1229545),\n",
       " (18165, 1.1225642),\n",
       " (16284, 1.1225299),\n",
       " (12723, 1.1225289),\n",
       " (19383, 1.1224515),\n",
       " (14188, 1.1219132),\n",
       " (12021, 1.1211314),\n",
       " (5750, 1.1206263),\n",
       " (13245, 1.1205413),\n",
       " (18737, 1.120141),\n",
       " (20132, 1.1196516),\n",
       " (2734, 1.1183288),\n",
       " (21000, 1.1181337),\n",
       " (3403, 1.1177676),\n",
       " (8670, 1.1173172),\n",
       " (7170, 1.1163273),\n",
       " (9388, 1.1157146),\n",
       " (3415, 1.1155064),\n",
       " (5816, 1.1151392),\n",
       " (12649, 1.1146308),\n",
       " (1274, 1.1143566),\n",
       " (15769, 1.1139915),\n",
       " (3512, 1.1129122),\n",
       " (13581, 1.1120296),\n",
       " (16688, 1.110726),\n",
       " (5939, 1.1100088),\n",
       " (9557, 1.1093163),\n",
       " (21760, 1.1092203),\n",
       " (8669, 1.1087248),\n",
       " (1249, 1.1084089),\n",
       " (1785, 1.1077808),\n",
       " (8371, 1.1069971),\n",
       " (771, 1.1066245),\n",
       " (3557, 1.1065385),\n",
       " (20021, 1.1065243),\n",
       " (1090, 1.1052344),\n",
       " (15643, 1.1048841),\n",
       " (15848, 1.1047356),\n",
       " (2484, 1.1042964),\n",
       " (20831, 1.1039395),\n",
       " (22664, 1.1035914),\n",
       " (7827, 1.1033291),\n",
       " (16240, 1.1029288),\n",
       " (18545, 1.1028243),\n",
       " (1146, 1.1027203),\n",
       " (3150, 1.1020987),\n",
       " (14753, 1.1019946),\n",
       " (12900, 1.1018281),\n",
       " (418, 1.1008705),\n",
       " (5591, 1.0993335),\n",
       " (872, 1.0989288),\n",
       " (1088, 1.0980442),\n",
       " (6051, 1.0977087),\n",
       " (1794, 1.0974798),\n",
       " (14019, 1.0968062),\n",
       " (4255, 1.0950031),\n",
       " (13466, 1.0949652),\n",
       " (17373, 1.0949383),\n",
       " (13216, 1.0949029),\n",
       " (323, 1.0946531),\n",
       " (15539, 1.0939827),\n",
       " (20865, 1.0927386),\n",
       " (11364, 1.0921677),\n",
       " (1660, 1.0921649),\n",
       " (14542, 1.0919063),\n",
       " (17684, 1.0915226),\n",
       " (4393, 1.0904405),\n",
       " (1977, 1.0900738),\n",
       " (7757, 1.0898718),\n",
       " (832, 1.089633),\n",
       " (3632, 1.0887362),\n",
       " (5, 1.0885227),\n",
       " (13854, 1.0878148),\n",
       " (7095, 1.0874717),\n",
       " (5609, 1.0869354),\n",
       " (2933, 1.0867785),\n",
       " (2496, 1.0862727),\n",
       " (7177, 1.0862466),\n",
       " (554, 1.0861458),\n",
       " (5522, 1.08533),\n",
       " (18488, 1.0836707),\n",
       " (13313, 1.0828024),\n",
       " (1202, 1.0826817),\n",
       " (4308, 1.0825312),\n",
       " (11690, 1.0824478),\n",
       " (696, 1.0818822),\n",
       " (5993, 1.0818088),\n",
       " (2779, 1.0809603),\n",
       " (18738, 1.0807899),\n",
       " (1079, 1.080355),\n",
       " (8445, 1.0800886),\n",
       " (14819, 1.0798994),\n",
       " (21479, 1.0798576),\n",
       " (1252, 1.0796033),\n",
       " (10655, 1.0783255),\n",
       " (4686, 1.0782387),\n",
       " (9698, 1.0781857),\n",
       " (12661, 1.0776954),\n",
       " (6655, 1.0773156),\n",
       " (870, 1.0770319),\n",
       " (13398, 1.0769553),\n",
       " (1129, 1.0762105),\n",
       " (3367, 1.0757573),\n",
       " (98, 1.0753441),\n",
       " (1226, 1.07503),\n",
       " (13710, 1.0746222),\n",
       " (16564, 1.0745729),\n",
       " (3784, 1.0745556),\n",
       " (11030, 1.0743818),\n",
       " (18577, 1.0732356),\n",
       " (10828, 1.0730557),\n",
       " (17834, 1.072929),\n",
       " (3723, 1.0729071),\n",
       " (3903, 1.0724447),\n",
       " (19954, 1.0721867),\n",
       " (2595, 1.0711861),\n",
       " (16883, 1.0710857),\n",
       " (9130, 1.0709847),\n",
       " (8673, 1.0708475),\n",
       " (21357, 1.0708076),\n",
       " (11753, 1.0701745),\n",
       " (2939, 1.0701733),\n",
       " (13517, 1.0701491),\n",
       " (2072, 1.0700954),\n",
       " (12423, 1.069326),\n",
       " (19994, 1.0690058),\n",
       " (353, 1.0687271),\n",
       " (21577, 1.0681503),\n",
       " (18126, 1.0675472),\n",
       " (15620, 1.0674658),\n",
       " (271, 1.06735),\n",
       " (1893, 1.0660957),\n",
       " (11207, 1.0660781),\n",
       " (15704, 1.0658438),\n",
       " (3160, 1.0656908),\n",
       " (2555, 1.0654941),\n",
       " (5204, 1.0651925),\n",
       " (1329, 1.0646935),\n",
       " (12954, 1.064503),\n",
       " (19380, 1.0633538),\n",
       " (5047, 1.0629194),\n",
       " (765, 1.0626193),\n",
       " (20366, 1.062403),\n",
       " (290, 1.0618019),\n",
       " (10347, 1.0615625),\n",
       " (11843, 1.0613502),\n",
       " (5330, 1.0612667),\n",
       " (4784, 1.0609493),\n",
       " (6050, 1.0606035),\n",
       " (4837, 1.0605841),\n",
       " (14360, 1.0600761),\n",
       " (13557, 1.0594215),\n",
       " (18314, 1.0593529),\n",
       " (21481, 1.0591578),\n",
       " (2419, 1.0588837),\n",
       " (10446, 1.0581858),\n",
       " (6294, 1.0579314),\n",
       " (19759, 1.0578188),\n",
       " (16918, 1.057704),\n",
       " (13020, 1.0576664),\n",
       " (14523, 1.0575757),\n",
       " (1002, 1.0573506),\n",
       " (11211, 1.0573272),\n",
       " (663, 1.0567315),\n",
       " (5389, 1.0563256),\n",
       " (9588, 1.0560195),\n",
       " (2435, 1.0549409),\n",
       " (12696, 1.0546596),\n",
       " (22390, 1.0546066),\n",
       " (11829, 1.053802),\n",
       " (12945, 1.0536512),\n",
       " (18704, 1.0533847),\n",
       " (7719, 1.0533755),\n",
       " (16636, 1.0532924),\n",
       " (6750, 1.0532918),\n",
       " (13777, 1.0530417),\n",
       " (2329, 1.0529339),\n",
       " (158, 1.0527444),\n",
       " (13725, 1.0518788),\n",
       " (4322, 1.0516601),\n",
       " (1287, 1.0515199),\n",
       " (18773, 1.0515157),\n",
       " (10294, 1.0514761),\n",
       " (4323, 1.0513935),\n",
       " (706, 1.051132),\n",
       " (3355, 1.0508866),\n",
       " (21482, 1.0504369),\n",
       " (1865, 1.0500851),\n",
       " (7097, 1.0500032),\n",
       " (21705, 1.0497342),\n",
       " (10439, 1.0492842),\n",
       " (14418, 1.0492439),\n",
       " (6443, 1.0491607),\n",
       " (3262, 1.0489228),\n",
       " (869, 1.0488751),\n",
       " (16867, 1.0488584),\n",
       " (10298, 1.0485922),\n",
       " (2205, 1.0483834),\n",
       " (12319, 1.0481484),\n",
       " (13864, 1.0480169),\n",
       " (3602, 1.0475146),\n",
       " (4451, 1.0469666),\n",
       " (6495, 1.0469165),\n",
       " (10223, 1.0467908),\n",
       " (1637, 1.0466897),\n",
       " (6500, 1.0461397),\n",
       " (19324, 1.0460352),\n",
       " (15666, 1.0458721),\n",
       " (12047, 1.0445762),\n",
       " (23135, 1.0442948),\n",
       " (9358, 1.0438069),\n",
       " (21275, 1.0434015),\n",
       " (4745, 1.0430799),\n",
       " (6603, 1.0429634),\n",
       " (480, 1.0425835),\n",
       " (18819, 1.0419493),\n",
       " (18853, 1.0412892),\n",
       " (6801, 1.0412338),\n",
       " (6382, 1.0412323),\n",
       " (379, 1.0411149),\n",
       " (17877, 1.0410055),\n",
       " (4991, 1.0408189),\n",
       " (2444, 1.0406445),\n",
       " (10560, 1.0400075),\n",
       " (13401, 1.0397248),\n",
       " (3715, 1.0392671),\n",
       " (5601, 1.0390742),\n",
       " (6362, 1.0390388),\n",
       " (1979, 1.0390141),\n",
       " (3555, 1.038844),\n",
       " (13080, 1.0383809),\n",
       " (385, 1.0382278),\n",
       " (12826, 1.0382123),\n",
       " (1616, 1.0376339),\n",
       " (8754, 1.0376263),\n",
       " (83, 1.0373936),\n",
       " (110, 1.0373883),\n",
       " (2, 1.037255),\n",
       " (1796, 1.0367849),\n",
       " (20861, 1.0365328),\n",
       " (16597, 1.0361242),\n",
       " (21021, 1.0361022),\n",
       " (3061, 1.0358349),\n",
       " (20137, 1.0358026),\n",
       " (4492, 1.0347846),\n",
       " (7302, 1.034759),\n",
       " (7306, 1.0341849),\n",
       " (2446, 1.0341101),\n",
       " (8306, 1.034032),\n",
       " (21978, 1.0337911),\n",
       " (2443, 1.0328941),\n",
       " (10991, 1.0327401),\n",
       " (10873, 1.0327394),\n",
       " (871, 1.0326998),\n",
       " (1466, 1.032645),\n",
       " (14596, 1.0323858),\n",
       " (19879, 1.0320561),\n",
       " (19363, 1.0313795),\n",
       " (1253, 1.0309883),\n",
       " (9978, 1.030794),\n",
       " (14283, 1.0305313),\n",
       " (728, 1.0303106),\n",
       " (5039, 1.0300479),\n",
       " (8119, 1.0299511),\n",
       " (3884, 1.0296257),\n",
       " (5593, 1.029547),\n",
       " (16053, 1.0291308),\n",
       " (1841, 1.0287566),\n",
       " (9067, 1.028642),\n",
       " (1144, 1.0284959),\n",
       " (1490, 1.0284914),\n",
       " (12343, 1.0279403),\n",
       " (5207, 1.0278757),\n",
       " (2694, 1.0274104),\n",
       " (7143, 1.0271354),\n",
       " (21345, 1.0266557),\n",
       " (14844, 1.0265684),\n",
       " (20875, 1.0264769),\n",
       " (16592, 1.0263959),\n",
       " (12412, 1.0256675),\n",
       " (508, 1.0251079),\n",
       " (6041, 1.0243531),\n",
       " (18800, 1.0240816),\n",
       " (18705, 1.0236647),\n",
       " (3180, 1.023547),\n",
       " (17542, 1.0229316),\n",
       " (2831, 1.0227735),\n",
       " (13339, 1.0222734),\n",
       " (12688, 1.0214285),\n",
       " (3147, 1.0212319),\n",
       " (18984, 1.020993),\n",
       " (7088, 1.0209738),\n",
       " (12732, 1.0208362),\n",
       " (4792, 1.0206693),\n",
       " (14155, 1.020625),\n",
       " (6413, 1.0204498),\n",
       " (2417, 1.0202966),\n",
       " (23444, 1.0202384),\n",
       " (14648, 1.0196286),\n",
       " (560, 1.0192671),\n",
       " (764, 1.0190146),\n",
       " (14039, 1.0189171),\n",
       " (18335, 1.0189021),\n",
       " (1409, 1.0188248),\n",
       " (20826, 1.0186496),\n",
       " (3417, 1.0185564),\n",
       " (3635, 1.0181857),\n",
       " (11501, 1.0179896),\n",
       " (9338, 1.0179833),\n",
       " (19362, 1.0175184),\n",
       " (21356, 1.0175054),\n",
       " (4434, 1.0165519),\n",
       " (3519, 1.0162059),\n",
       " (11838, 1.0158259),\n",
       " (17936, 1.0157386),\n",
       " (7303, 1.015383),\n",
       " (1211, 1.015331),\n",
       " (19319, 1.0151949),\n",
       " (13886, 1.0150075),\n",
       " (3772, 1.0150001),\n",
       " (10466, 1.0148156),\n",
       " (9169, 1.0146692),\n",
       " (7980, 1.0144818),\n",
       " (9382, 1.0143678),\n",
       " (2285, 1.013865),\n",
       " (14999, 1.0136156),\n",
       " (3643, 1.0133601),\n",
       " (7408, 1.0132982),\n",
       " (13978, 1.0131737),\n",
       " (2814, 1.0129939),\n",
       " (3618, 1.0128362),\n",
       " (17512, 1.0124996),\n",
       " (1020, 1.012313),\n",
       " (4247, 1.0120187),\n",
       " (1219, 1.0119953),\n",
       " (7385, 1.0119929),\n",
       " (14981, 1.0116271),\n",
       " (3741, 1.0115932),\n",
       " (2225, 1.011533),\n",
       " (18344, 1.0114912),\n",
       " (806, 1.0113868),\n",
       " (14385, 1.0111398),\n",
       " (12645, 1.0110059),\n",
       " (2042, 1.0108793),\n",
       " (12794, 1.0107999),\n",
       " (27, 1.0104249),\n",
       " (372, 1.0102124),\n",
       " (2895, 1.0098608),\n",
       " (614, 1.0096724),\n",
       " (5489, 1.0096433),\n",
       " (7096, 1.0094118),\n",
       " (4355, 1.0093048),\n",
       " (5795, 1.0092089),\n",
       " (12066, 1.0090427),\n",
       " (13634, 1.0089699),\n",
       " (19644, 1.008591),\n",
       " (4680, 1.0081336),\n",
       " (6762, 1.0078835),\n",
       " (3720, 1.007743),\n",
       " (22542, 1.0072911),\n",
       " (2534, 1.0070404),\n",
       " (146, 1.0069342),\n",
       " (22400, 1.0066936),\n",
       " (1242, 1.0065714),\n",
       " (3978, 1.0064045),\n",
       " (21202, 1.0056709),\n",
       " (19241, 1.0054826),\n",
       " (629, 1.0052472),\n",
       " (13242, 1.0051798),\n",
       " (17865, 1.0045753),\n",
       " (20974, 1.0043744),\n",
       " (11058, 1.0042591),\n",
       " (7762, 1.0037588),\n",
       " (6478, 1.0036868),\n",
       " (410, 1.0034488),\n",
       " (5714, 1.0032414),\n",
       " (1498, 1.003062),\n",
       " (8332, 1.0029991),\n",
       " (2455, 1.002919),\n",
       " (16920, 1.002919),\n",
       " (1309, 1.0027245),\n",
       " (3015, 1.0026729),\n",
       " (14668, 1.0023777),\n",
       " (4646, 1.0023569),\n",
       " (3922, 1.0021396),\n",
       " (11670, 1.0021327),\n",
       " (22502, 1.0014973),\n",
       " (365, 1.0009537),\n",
       " (3569, 1.0007287),\n",
       " (1042, 1.0005542),\n",
       " (18571, 1.0005348),\n",
       " (78, 1.0001211),\n",
       " (1288, 1.0000124),\n",
       " (628, 0.9996803),\n",
       " (3938, 0.9996569),\n",
       " (7709, 0.9996488),\n",
       " (4912, 0.99948156),\n",
       " (7564, 0.9993809),\n",
       " (12995, 0.999207),\n",
       " (22613, 0.9991133),\n",
       " (285, 0.9990952),\n",
       " (12384, 0.9989478),\n",
       " (3478, 0.9988566),\n",
       " (1722, 0.9987013),\n",
       " (7600, 0.99851465),\n",
       " (21600, 0.9983742),\n",
       " (9203, 0.9982641),\n",
       " (8330, 0.99760735),\n",
       " (12547, 0.9975027),\n",
       " (20821, 0.99748224),\n",
       " (15081, 0.9974667),\n",
       " (6584, 0.997354),\n",
       " (2341, 0.9973059),\n",
       " (12660, 0.9970191),\n",
       " (11350, 0.99689466),\n",
       " (15977, 0.99674815),\n",
       " (5278, 0.996649),\n",
       " (22753, 0.9962798),\n",
       " (14109, 0.9962377),\n",
       " (2986, 0.99617475),\n",
       " (2274, 0.99614733),\n",
       " (6314, 0.99580735),\n",
       " (483, 0.9957206),\n",
       " (8963, 0.99491537),\n",
       " (7869, 0.9948646),\n",
       " (3455, 0.9947867),\n",
       " (14665, 0.9947569),\n",
       " (324, 0.99468565),\n",
       " (1523, 0.9944722),\n",
       " (6244, 0.99431247),\n",
       " (151, 0.9940373),\n",
       " (5439, 0.993991),\n",
       " (3314, 0.9939904),\n",
       " (2630, 0.99328566),\n",
       " (1406, 0.99314886),\n",
       " (901, 0.9926281),\n",
       " (10435, 0.99262774),\n",
       " (1281, 0.9925514),\n",
       " (1289, 0.9920895),\n",
       " (5445, 0.99173254),\n",
       " (23218, 0.99158466),\n",
       " (6455, 0.9913909),\n",
       " (21984, 0.99130845),\n",
       " (1767, 0.9912629),\n",
       " (1592, 0.99117863),\n",
       " (7490, 0.9911151),\n",
       " (5416, 0.99098575),\n",
       " (22597, 0.99092174),\n",
       " (2401, 0.9905808),\n",
       " (6793, 0.9904252),\n",
       " (16237, 0.99035436),\n",
       " (16014, 0.9902591),\n",
       " (23064, 0.9902326),\n",
       " (7462, 0.99017006),\n",
       " (3341, 0.9895304),\n",
       " (14854, 0.98910064),\n",
       " (3027, 0.98879874),\n",
       " (301, 0.9886464),\n",
       " (3988, 0.9881208),\n",
       " (10910, 0.9880178),\n",
       " (12377, 0.9879869),\n",
       " (6105, 0.98789936),\n",
       " (4153, 0.9878106),\n",
       " (8038, 0.9874928),\n",
       " (21286, 0.98743486),\n",
       " (15587, 0.98740995),\n",
       " (5912, 0.98730636),\n",
       " (8585, 0.9872081),\n",
       " (2244, 0.9870926),\n",
       " (8071, 0.98703015),\n",
       " (2022, 0.9863001),\n",
       " (12862, 0.9862642),\n",
       " (3859, 0.9862017),\n",
       " (8327, 0.9861757),\n",
       " (17894, 0.9861274),\n",
       " (8750, 0.9860036),\n",
       " (294, 0.9851073),\n",
       " (17784, 0.9848641),\n",
       " (14439, 0.9844455),\n",
       " (3944, 0.9841053),\n",
       " (9426, 0.9840882),\n",
       " (13550, 0.98400855),\n",
       " (13547, 0.98341614),\n",
       " (7405, 0.98320866),\n",
       " (4390, 0.98263043),\n",
       " (1159, 0.98193604),\n",
       " (1470, 0.98181623),\n",
       " (14463, 0.9813744),\n",
       " (18166, 0.9813701),\n",
       " (11923, 0.9812888),\n",
       " (20910, 0.98118293),\n",
       " (548, 0.980845),\n",
       " (12282, 0.9804609),\n",
       " (1128, 0.98039496),\n",
       " (1632, 0.98038805),\n",
       " (8031, 0.9800836),\n",
       " (9369, 0.9797223),\n",
       " (13560, 0.97953004),\n",
       " (9959, 0.97942954),\n",
       " (3422, 0.9793042),\n",
       " (7126, 0.9790905),\n",
       " (20967, 0.97906375),\n",
       " (6199, 0.9786844),\n",
       " (12814, 0.9786408),\n",
       " (1060, 0.9786398),\n",
       " (5137, 0.9785819),\n",
       " (13924, 0.9780354),\n",
       " (4075, 0.97793365),\n",
       " (626, 0.97762305),\n",
       " (2726, 0.97761226),\n",
       " (11695, 0.97736865),\n",
       " (21006, 0.976966),\n",
       " (1449, 0.97690207),\n",
       " (4864, 0.9764707),\n",
       " (1344, 0.9764442),\n",
       " (4320, 0.9764037),\n",
       " (12575, 0.97623336),\n",
       " (6638, 0.97619224),\n",
       " (18779, 0.97604394),\n",
       " (13622, 0.9757513),\n",
       " (2240, 0.97569704),\n",
       " (22653, 0.97564733),\n",
       " (968, 0.9755723),\n",
       " (3837, 0.97541326),\n",
       " (3172, 0.97540176),\n",
       " (2232, 0.97528654),\n",
       " (19520, 0.97513187),\n",
       " (16346, 0.97482115),\n",
       " (7562, 0.9748026),\n",
       " (1149, 0.9746278),\n",
       " (1147, 0.974542),\n",
       " (4542, 0.9745021),\n",
       " (2530, 0.97385913),\n",
       " (1612, 0.97367),\n",
       " (322, 0.9736518),\n",
       " (18910, 0.97363234),\n",
       " (2307, 0.9735915),\n",
       " (2817, 0.9735745),\n",
       " (16327, 0.97336817),\n",
       " (17727, 0.9733115),\n",
       " (11693, 0.9731709),\n",
       " (2449, 0.9731244),\n",
       " (689, 0.97278154),\n",
       " (4163, 0.97266257),\n",
       " (2099, 0.97258586),\n",
       " (16690, 0.972431),\n",
       " (14254, 0.9724179),\n",
       " (22053, 0.97218996),\n",
       " (8067, 0.97185135),\n",
       " (15336, 0.9717771),\n",
       " (5748, 0.9714223),\n",
       " (22254, 0.97140586),\n",
       " (13148, 0.9713013),\n",
       " (2068, 0.97116953),\n",
       " (11439, 0.971084),\n",
       " (15570, 0.970664),\n",
       " (7207, 0.9706606),\n",
       " (8881, 0.9706484),\n",
       " (15449, 0.9705285),\n",
       " (20362, 0.9703845),\n",
       " (10754, 0.97028935),\n",
       " (1546, 0.97012997),\n",
       " (13918, 0.96991503),\n",
       " (4631, 0.9698922),\n",
       " (10199, 0.9695075),\n",
       " (13592, 0.9694158),\n",
       " (23287, 0.96874374),\n",
       " (18700, 0.9682921),\n",
       " (4424, 0.96826935),\n",
       " (3558, 0.9682542),\n",
       " (17589, 0.9681288),\n",
       " (9577, 0.9678247),\n",
       " (125, 0.967819),\n",
       " (1182, 0.9677134),\n",
       " (16217, 0.96769184),\n",
       " (7351, 0.9674542),\n",
       " (16145, 0.9674078),\n",
       " (5620, 0.96738034),\n",
       " (342, 0.9672488),\n",
       " (2380, 0.9668978),\n",
       " (22923, 0.9668838),\n",
       " (875, 0.966854),\n",
       " (829, 0.96681094),\n",
       " (1801, 0.9666109),\n",
       " (6118, 0.966395),\n",
       " (21729, 0.966354),\n",
       " (310, 0.9662013),\n",
       " (3906, 0.9659976),\n",
       " (21209, 0.96598685),\n",
       " (12722, 0.9659489),\n",
       " (16563, 0.9658827),\n",
       " (16633, 0.9657766),\n",
       " (18279, 0.9654439),\n",
       " (14994, 0.9645498),\n",
       " (384, 0.9642743),\n",
       " (1218, 0.96423626),\n",
       " (1049, 0.96406806),\n",
       " (1727, 0.96398383),\n",
       " (16148, 0.96383953),\n",
       " (4984, 0.96340907),\n",
       " (9118, 0.9633781),\n",
       " (22066, 0.9633373),\n",
       " (3608, 0.9630269),\n",
       " (9242, 0.96282756),\n",
       " (1187, 0.9626647),\n",
       " (6740, 0.9623459),\n",
       " (634, 0.9621386),\n",
       " (7101, 0.9621271),\n",
       " (11677, 0.962125),\n",
       " (2666, 0.96203077),\n",
       " (7406, 0.9620192),\n",
       " (17849, 0.961558),\n",
       " (6527, 0.96148527),\n",
       " (23305, 0.96140605),\n",
       " (10903, 0.9611619),\n",
       " (13263, 0.9610412),\n",
       " (12019, 0.9610237),\n",
       " (19204, 0.96074003),\n",
       " (19354, 0.96069133),\n",
       " (12806, 0.9606107),\n",
       " (12786, 0.9603957),\n",
       " (18445, 0.9603331),\n",
       " (4161, 0.96029824),\n",
       " (15708, 0.9602074),\n",
       " (15802, 0.96000755),\n",
       " (5525, 0.95998603),\n",
       " (21391, 0.9597134),\n",
       " (7314, 0.9596779),\n",
       " (20142, 0.9594544),\n",
       " (11078, 0.9593077),\n",
       " (2670, 0.9591008),\n",
       " (14214, 0.95907766),\n",
       " (5034, 0.95879644),\n",
       " (5146, 0.9585935),\n",
       " (1326, 0.9581568),\n",
       " (2598, 0.9579529),\n",
       " (14389, 0.95792425),\n",
       " (6798, 0.95784616),\n",
       " (4438, 0.9575802),\n",
       " (13881, 0.9575102),\n",
       " (13102, 0.9574131),\n",
       " (11144, 0.95736015),\n",
       " (17534, 0.95731217),\n",
       " (12855, 0.9571833),\n",
       " (758, 0.95713353),\n",
       " (13203, 0.95700306),\n",
       " (6984, 0.9568926),\n",
       " (5711, 0.95653105),\n",
       " (15782, 0.95645607),\n",
       " (6007, 0.95635366),\n",
       " (11701, 0.95630705),\n",
       " (492, 0.95625657),\n",
       " (13417, 0.9561869),\n",
       " (16296, 0.9559356),\n",
       " (16717, 0.95586765),\n",
       " (18994, 0.9557996),\n",
       " (21987, 0.95549166),\n",
       " (2422, 0.95523703),\n",
       " (14972, 0.95523536),\n",
       " (15835, 0.95514214),\n",
       " (7915, 0.954936),\n",
       " (14503, 0.95472306),\n",
       " (1445, 0.9544916),\n",
       " (79, 0.95442057),\n",
       " (735, 0.9541002),\n",
       " (11999, 0.95405173),\n",
       " (13034, 0.9539326),\n",
       " (1017, 0.95350534),\n",
       " (19982, 0.9535011),\n",
       " (3037, 0.9534168),\n",
       " (638, 0.95327663),\n",
       " (3690, 0.95320404),\n",
       " (14038, 0.9531285),\n",
       " (2765, 0.9530501),\n",
       " (17019, 0.95298636),\n",
       " (9028, 0.9526051),\n",
       " (1840, 0.9525956),\n",
       " (15725, 0.9525205),\n",
       " (19531, 0.9525104),\n",
       " (8451, 0.9523273),\n",
       " (11998, 0.95226413),\n",
       " (518, 0.9516128),\n",
       " (9085, 0.9515685),\n",
       " (8144, 0.95123446),\n",
       " (8195, 0.9511124),\n",
       " (1267, 0.95110905),\n",
       " (11544, 0.9510741),\n",
       " (13071, 0.9508574),\n",
       " (3261, 0.9507879),\n",
       " (4254, 0.9507375),\n",
       " (13183, 0.950465),\n",
       " (10579, 0.9503882),\n",
       " (16493, 0.9502097),\n",
       " (4535, 0.9501742),\n",
       " (8953, 0.94997644),\n",
       " (665, 0.9495298),\n",
       " (888, 0.94947356),\n",
       " (11314, 0.9494308),\n",
       " (12711, 0.94934744),\n",
       " (10396, 0.9492483),\n",
       " (5683, 0.9491322),\n",
       " (5264, 0.94913024),\n",
       " (12922, 0.94897884),\n",
       " (1086, 0.94886166),\n",
       " (2289, 0.9488157),\n",
       " (20273, 0.9484889),\n",
       " (11499, 0.9484136),\n",
       " (2972, 0.9482937),\n",
       " (5437, 0.9480555),\n",
       " (1382, 0.9479612),\n",
       " (10105, 0.9475498),\n",
       " (7592, 0.9475051),\n",
       " (20219, 0.9474473),\n",
       " (11608, 0.94731736),\n",
       " (8564, 0.9470641),\n",
       " (20414, 0.94671834),\n",
       " (502, 0.9466047),\n",
       " (4870, 0.9464572),\n",
       " (22342, 0.9461099),\n",
       " (9542, 0.94607484),\n",
       " (975, 0.9457196),\n",
       " (19650, 0.9457123),\n",
       " (7407, 0.9454961),\n",
       " (12420, 0.9450677),\n",
       " (8697, 0.94494313),\n",
       " (14495, 0.94481826),\n",
       " (13757, 0.94471157),\n",
       " (16252, 0.94452167),\n",
       " (6125, 0.9444839),\n",
       " (8122, 0.9443988),\n",
       " (3339, 0.9443737),\n",
       " (20874, 0.9441266),\n",
       " (1773, 0.94409597),\n",
       " (1424, 0.94408715),\n",
       " (5009, 0.9440112),\n",
       " (15633, 0.9439685),\n",
       " (4617, 0.94392276),\n",
       " (2216, 0.94357026),\n",
       " (16451, 0.9434948),\n",
       " (466, 0.94349205),\n",
       " (1696, 0.9432984),\n",
       " (1922, 0.94326866),\n",
       " (4109, 0.94300604),\n",
       " (21380, 0.9429394),\n",
       " (22482, 0.9426519),\n",
       " (2280, 0.9426152),\n",
       " (427, 0.94211817),\n",
       " (1043, 0.94203794),\n",
       " (20840, 0.941911),\n",
       " (6030, 0.9417559),\n",
       " (3313, 0.94159514),\n",
       " (15448, 0.94150025),\n",
       " (9425, 0.94135034),\n",
       " (16704, 0.9410454),\n",
       " (2488, 0.9409887),\n",
       " (22475, 0.94091386),\n",
       " (5099, 0.94085217),\n",
       " (13647, 0.9406557),\n",
       " (15813, 0.9404102),\n",
       " (4332, 0.9403495),\n",
       " (12710, 0.9403457),\n",
       " (13716, 0.9401276),\n",
       " (17588, 0.9400878),\n",
       " (16206, 0.93985665),\n",
       " (7475, 0.93977934),\n",
       " (8483, 0.93962324),\n",
       " (15604, 0.93949234),\n",
       " (563, 0.9394395),\n",
       " (10297, 0.9393811),\n",
       " (18494, 0.9391197),\n",
       " (3220, 0.939059),\n",
       " (5742, 0.939013),\n",
       " (3949, 0.93896043),\n",
       " (713, 0.9388151),\n",
       " (7886, 0.9387502),\n",
       " (17799, 0.93864894),\n",
       " (10757, 0.9385902),\n",
       " (6633, 0.9381286),\n",
       " (16215, 0.9380025),\n",
       " (7175, 0.93796325),\n",
       " (474, 0.9377761),\n",
       " (3604, 0.9376818),\n",
       " (16047, 0.9376083),\n",
       " (14543, 0.9372896),\n",
       " (15450, 0.93714637),\n",
       " (11312, 0.9369857),\n",
       " (18855, 0.93688035),\n",
       " (918, 0.93670857),\n",
       " (2812, 0.93664193),\n",
       " (23593, 0.9365494),\n",
       " (21785, 0.93641675),\n",
       " (3254, 0.9363296),\n",
       " (12787, 0.9361682),\n",
       " (12353, 0.9359153),\n",
       " (12970, 0.9356233),\n",
       " (15263, 0.93541074),\n",
       " (2333, 0.9353236),\n",
       " (17942, 0.9349092),\n",
       " (18977, 0.93479526),\n",
       " (6372, 0.93466073),\n",
       " (4605, 0.9346161),\n",
       " (17930, 0.9345378),\n",
       " (2414, 0.93446565),\n",
       " (6947, 0.9343405),\n",
       " (4307, 0.9341554),\n",
       " (4076, 0.93414164),\n",
       " (19060, 0.93398356),\n",
       " (5430, 0.9338796),\n",
       " (6088, 0.93375915),\n",
       " (13579, 0.93374),\n",
       " (16682, 0.93362904),\n",
       " (4251, 0.93345),\n",
       " (6466, 0.93340707),\n",
       " (13500, 0.9333905),\n",
       " (17511, 0.93337846),\n",
       " (4289, 0.93330425),\n",
       " (2317, 0.9329981),\n",
       " (4391, 0.9329894),\n",
       " (4640, 0.93286645),\n",
       " (18544, 0.93279624),\n",
       " (14301, 0.9327802),\n",
       " (6940, 0.9326266),\n",
       " (17014, 0.9325115),\n",
       " (6279, 0.93249375),\n",
       " (7173, 0.93205607),\n",
       " (7664, 0.9320389),\n",
       " (2840, 0.9318624),\n",
       " (6715, 0.93179107),\n",
       " (21674, 0.9317074),\n",
       " (16527, 0.93160915),\n",
       " (19425, 0.9312464),\n",
       " (5083, 0.9311595),\n",
       " (16900, 0.9309911),\n",
       " (5470, 0.9309156),\n",
       " (3008, 0.930658),\n",
       " (7271, 0.93047875),\n",
       " (9589, 0.93040496),\n",
       " (4090, 0.930385),\n",
       " ...]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_top_n(fism, user_id, user_rated_items, batch_size=4096, top_n=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('01pqZKkmGrZvh5OD0qh8DGaeGbCpEjGT',\n",
       " [53541.0, 291027.0, 138234.0, 66036.0, 62454.0, 346291.0])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_users_in_train = []\n",
    "for user_session_id, items in test_users:\n",
    "    if user_idx_dict.__contains__(user_session_id):\n",
    "        test_users_in_train.append((user_session_id, items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_users_in_train.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7823613086770982, 0.06401137980085349)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_train_rate = in_train_count/total_items\n",
    "hit_rate = count_hit/total_items\n",
    "in_train_rate, hit_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_item_ids = {rec_item[0] for rec_item in rec_top_n}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 257,\n",
       " 258,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 269,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 292,\n",
       " 293,\n",
       " 294,\n",
       " 295,\n",
       " 296,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 301,\n",
       " 302,\n",
       " 303,\n",
       " 304,\n",
       " 305,\n",
       " 306,\n",
       " 307,\n",
       " 308,\n",
       " 309,\n",
       " 310,\n",
       " 311,\n",
       " 312,\n",
       " 313,\n",
       " 314,\n",
       " 315,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 319,\n",
       " 320,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 325,\n",
       " 326,\n",
       " 327,\n",
       " 328,\n",
       " 329,\n",
       " 330,\n",
       " 331,\n",
       " 332,\n",
       " 333,\n",
       " 334,\n",
       " 335,\n",
       " 336,\n",
       " 337,\n",
       " 338,\n",
       " 339,\n",
       " 340,\n",
       " 341,\n",
       " 342,\n",
       " 343,\n",
       " 344,\n",
       " 345,\n",
       " 346,\n",
       " 347,\n",
       " 348,\n",
       " 349,\n",
       " 350,\n",
       " 351,\n",
       " 352,\n",
       " 353,\n",
       " 354,\n",
       " 355,\n",
       " 356,\n",
       " 357,\n",
       " 358,\n",
       " 359,\n",
       " 360,\n",
       " 361,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 366,\n",
       " 367,\n",
       " 368,\n",
       " 369,\n",
       " 370,\n",
       " 371,\n",
       " 372,\n",
       " 373,\n",
       " 374,\n",
       " 375,\n",
       " 376,\n",
       " 377,\n",
       " 378,\n",
       " 379,\n",
       " 380,\n",
       " 381,\n",
       " 382,\n",
       " 383,\n",
       " 384,\n",
       " 385,\n",
       " 386,\n",
       " 387,\n",
       " 388,\n",
       " 389,\n",
       " 390,\n",
       " 391,\n",
       " 392,\n",
       " 393,\n",
       " 394,\n",
       " 395,\n",
       " 396,\n",
       " 397,\n",
       " 398,\n",
       " 399,\n",
       " 400,\n",
       " 401,\n",
       " 402,\n",
       " 403,\n",
       " 404,\n",
       " 405,\n",
       " 406,\n",
       " 407,\n",
       " 408,\n",
       " 409,\n",
       " 410,\n",
       " 411,\n",
       " 412,\n",
       " 413,\n",
       " 414,\n",
       " 415,\n",
       " 416,\n",
       " 417,\n",
       " 418,\n",
       " 419,\n",
       " 420,\n",
       " 421,\n",
       " 422,\n",
       " 423,\n",
       " 424,\n",
       " 425,\n",
       " 426,\n",
       " 427,\n",
       " 428,\n",
       " 429,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 433,\n",
       " 434,\n",
       " 435,\n",
       " 436,\n",
       " 437,\n",
       " 438,\n",
       " 439,\n",
       " 440,\n",
       " 441,\n",
       " 442,\n",
       " 443,\n",
       " 444,\n",
       " 445,\n",
       " 446,\n",
       " 447,\n",
       " 448,\n",
       " 449,\n",
       " 450,\n",
       " 451,\n",
       " 452,\n",
       " 453,\n",
       " 454,\n",
       " 455,\n",
       " 456,\n",
       " 457,\n",
       " 458,\n",
       " 459,\n",
       " 460,\n",
       " 461,\n",
       " 462,\n",
       " 463,\n",
       " 464,\n",
       " 465,\n",
       " 466,\n",
       " 467,\n",
       " 468,\n",
       " 469,\n",
       " 470,\n",
       " 471,\n",
       " 472,\n",
       " 473,\n",
       " 474,\n",
       " 475,\n",
       " 476,\n",
       " 477,\n",
       " 478,\n",
       " 479,\n",
       " 480,\n",
       " 481,\n",
       " 482,\n",
       " 483,\n",
       " 484,\n",
       " 485,\n",
       " 486,\n",
       " 487,\n",
       " 488,\n",
       " 489,\n",
       " 490,\n",
       " 491,\n",
       " 492,\n",
       " 493,\n",
       " 494,\n",
       " 495,\n",
       " 496,\n",
       " 497,\n",
       " 498,\n",
       " 499,\n",
       " 500,\n",
       " 501,\n",
       " 502,\n",
       " 503,\n",
       " 504,\n",
       " 505,\n",
       " 506,\n",
       " 507,\n",
       " 508,\n",
       " 509,\n",
       " 510,\n",
       " 511,\n",
       " 512,\n",
       " 513,\n",
       " 514,\n",
       " 515,\n",
       " 516,\n",
       " 517,\n",
       " 518,\n",
       " 519,\n",
       " 520,\n",
       " 521,\n",
       " 522,\n",
       " 523,\n",
       " 524,\n",
       " 525,\n",
       " 526,\n",
       " 527,\n",
       " 528,\n",
       " 529,\n",
       " 530,\n",
       " 531,\n",
       " 532,\n",
       " 533,\n",
       " 534,\n",
       " 535,\n",
       " 536,\n",
       " 537,\n",
       " 538,\n",
       " 539,\n",
       " 540,\n",
       " 541,\n",
       " 542,\n",
       " 543,\n",
       " 544,\n",
       " 545,\n",
       " 546,\n",
       " 547,\n",
       " 548,\n",
       " 549,\n",
       " 550,\n",
       " 551,\n",
       " 552,\n",
       " 553,\n",
       " 554,\n",
       " 555,\n",
       " 556,\n",
       " 557,\n",
       " 558,\n",
       " 559,\n",
       " 560,\n",
       " 561,\n",
       " 562,\n",
       " 563,\n",
       " 564,\n",
       " 565,\n",
       " 566,\n",
       " 567,\n",
       " 568,\n",
       " 569,\n",
       " 570,\n",
       " 571,\n",
       " 572,\n",
       " 573,\n",
       " 574,\n",
       " 575,\n",
       " 576,\n",
       " 577,\n",
       " 578,\n",
       " 579,\n",
       " 580,\n",
       " 581,\n",
       " 582,\n",
       " 583,\n",
       " 584,\n",
       " 585,\n",
       " 586,\n",
       " 587,\n",
       " 588,\n",
       " 589,\n",
       " 590,\n",
       " 591,\n",
       " 592,\n",
       " 593,\n",
       " 594,\n",
       " 595,\n",
       " 596,\n",
       " 597,\n",
       " 598,\n",
       " 599,\n",
       " 600,\n",
       " 601,\n",
       " 602,\n",
       " 603,\n",
       " 604,\n",
       " 605,\n",
       " 606,\n",
       " 607,\n",
       " 608,\n",
       " 609,\n",
       " 610,\n",
       " 611,\n",
       " 612,\n",
       " 613,\n",
       " 614,\n",
       " 615,\n",
       " 616,\n",
       " 617,\n",
       " 618,\n",
       " 619,\n",
       " 620,\n",
       " 621,\n",
       " 622,\n",
       " 623,\n",
       " 624,\n",
       " 625,\n",
       " 626,\n",
       " 627,\n",
       " 628,\n",
       " 629,\n",
       " 630,\n",
       " 631,\n",
       " 632,\n",
       " 633,\n",
       " 634,\n",
       " 635,\n",
       " 636,\n",
       " 637,\n",
       " 638,\n",
       " 639,\n",
       " 640,\n",
       " 641,\n",
       " 642,\n",
       " 643,\n",
       " 644,\n",
       " 645,\n",
       " 646,\n",
       " 647,\n",
       " 648,\n",
       " 649,\n",
       " 650,\n",
       " 651,\n",
       " 652,\n",
       " 653,\n",
       " 654,\n",
       " 655,\n",
       " 656,\n",
       " 657,\n",
       " 658,\n",
       " 659,\n",
       " 660,\n",
       " 661,\n",
       " 662,\n",
       " 663,\n",
       " 664,\n",
       " 665,\n",
       " 666,\n",
       " 667,\n",
       " 668,\n",
       " 669,\n",
       " 670,\n",
       " 671,\n",
       " 672,\n",
       " 673,\n",
       " 674,\n",
       " 675,\n",
       " 676,\n",
       " 677,\n",
       " 678,\n",
       " 679,\n",
       " 680,\n",
       " 681,\n",
       " 682,\n",
       " 683,\n",
       " 684,\n",
       " 685,\n",
       " 686,\n",
       " 687,\n",
       " 688,\n",
       " 689,\n",
       " 690,\n",
       " 691,\n",
       " 692,\n",
       " 693,\n",
       " 694,\n",
       " 695,\n",
       " 696,\n",
       " 697,\n",
       " 698,\n",
       " 699,\n",
       " 700,\n",
       " 701,\n",
       " 702,\n",
       " 703,\n",
       " 704,\n",
       " 705,\n",
       " 706,\n",
       " 707,\n",
       " 708,\n",
       " 709,\n",
       " 710,\n",
       " 711,\n",
       " 712,\n",
       " 713,\n",
       " 714,\n",
       " 715,\n",
       " 716,\n",
       " 717,\n",
       " 718,\n",
       " 719,\n",
       " 720,\n",
       " 721,\n",
       " 722,\n",
       " 723,\n",
       " 724,\n",
       " 725,\n",
       " 726,\n",
       " 727,\n",
       " 728,\n",
       " 729,\n",
       " 730,\n",
       " 731,\n",
       " 732,\n",
       " 733,\n",
       " 734,\n",
       " 735,\n",
       " 736,\n",
       " 737,\n",
       " 738,\n",
       " 739,\n",
       " 740,\n",
       " 741,\n",
       " 742,\n",
       " 743,\n",
       " 744,\n",
       " 745,\n",
       " 746,\n",
       " 747,\n",
       " 748,\n",
       " 749,\n",
       " 750,\n",
       " 751,\n",
       " 752,\n",
       " 753,\n",
       " 754,\n",
       " 755,\n",
       " 756,\n",
       " 757,\n",
       " 758,\n",
       " 759,\n",
       " 760,\n",
       " 761,\n",
       " 762,\n",
       " 763,\n",
       " 764,\n",
       " 765,\n",
       " 766,\n",
       " 767,\n",
       " 768,\n",
       " 769,\n",
       " 770,\n",
       " 771,\n",
       " 772,\n",
       " 773,\n",
       " 774,\n",
       " 775,\n",
       " 776,\n",
       " 777,\n",
       " 778,\n",
       " 779,\n",
       " 780,\n",
       " 781,\n",
       " 782,\n",
       " 783,\n",
       " 784,\n",
       " 785,\n",
       " 786,\n",
       " 787,\n",
       " 788,\n",
       " 789,\n",
       " 790,\n",
       " 791,\n",
       " 792,\n",
       " 793,\n",
       " 794,\n",
       " 795,\n",
       " 796,\n",
       " 797,\n",
       " 798,\n",
       " 799,\n",
       " 800,\n",
       " 801,\n",
       " 802,\n",
       " 803,\n",
       " 804,\n",
       " 805,\n",
       " 806,\n",
       " 807,\n",
       " 808,\n",
       " 809,\n",
       " 810,\n",
       " 811,\n",
       " 812,\n",
       " 813,\n",
       " 814,\n",
       " 815,\n",
       " 816,\n",
       " 817,\n",
       " 818,\n",
       " 819,\n",
       " 820,\n",
       " 821,\n",
       " 822,\n",
       " 823,\n",
       " 824,\n",
       " 825,\n",
       " 826,\n",
       " 827,\n",
       " 828,\n",
       " 829,\n",
       " 830,\n",
       " 831,\n",
       " 832,\n",
       " 833,\n",
       " 834,\n",
       " 835,\n",
       " 836,\n",
       " 837,\n",
       " 838,\n",
       " 839,\n",
       " 840,\n",
       " 841,\n",
       " 842,\n",
       " 843,\n",
       " 844,\n",
       " 845,\n",
       " 846,\n",
       " 847,\n",
       " 848,\n",
       " 849,\n",
       " 850,\n",
       " 851,\n",
       " 852,\n",
       " 853,\n",
       " 854,\n",
       " 855,\n",
       " 856,\n",
       " 857,\n",
       " 858,\n",
       " 859,\n",
       " 860,\n",
       " 861,\n",
       " 862,\n",
       " 863,\n",
       " 864,\n",
       " 865,\n",
       " 866,\n",
       " 867,\n",
       " 868,\n",
       " 869,\n",
       " 870,\n",
       " 871,\n",
       " 872,\n",
       " 873,\n",
       " 874,\n",
       " 875,\n",
       " 876,\n",
       " 877,\n",
       " 878,\n",
       " 879,\n",
       " 880,\n",
       " 881,\n",
       " 882,\n",
       " 883,\n",
       " 884,\n",
       " 885,\n",
       " 886,\n",
       " 887,\n",
       " 888,\n",
       " 889,\n",
       " 890,\n",
       " 891,\n",
       " 892,\n",
       " 893,\n",
       " 894,\n",
       " 895,\n",
       " 896,\n",
       " 897,\n",
       " 898,\n",
       " 899,\n",
       " 900,\n",
       " 901,\n",
       " 902,\n",
       " 903,\n",
       " 904,\n",
       " 905,\n",
       " 906,\n",
       " 907,\n",
       " 908,\n",
       " 909,\n",
       " 910,\n",
       " 911,\n",
       " 912,\n",
       " 913,\n",
       " 914,\n",
       " 915,\n",
       " 916,\n",
       " 917,\n",
       " 918,\n",
       " 919,\n",
       " 920,\n",
       " 921,\n",
       " 922,\n",
       " 923,\n",
       " 924,\n",
       " 925,\n",
       " 926,\n",
       " 927,\n",
       " 928,\n",
       " 929,\n",
       " 930,\n",
       " 931,\n",
       " 932,\n",
       " 933,\n",
       " 934,\n",
       " 935,\n",
       " 936,\n",
       " 937,\n",
       " 938,\n",
       " 939,\n",
       " 940,\n",
       " 941,\n",
       " 942,\n",
       " 943,\n",
       " 944,\n",
       " 945,\n",
       " 946,\n",
       " 947,\n",
       " 948,\n",
       " 949,\n",
       " 950,\n",
       " 951,\n",
       " 952,\n",
       " 953,\n",
       " 954,\n",
       " 955,\n",
       " 956,\n",
       " 957,\n",
       " 958,\n",
       " 959,\n",
       " 960,\n",
       " 961,\n",
       " 962,\n",
       " 963,\n",
       " 964,\n",
       " 965,\n",
       " 966,\n",
       " 967,\n",
       " 968,\n",
       " 969,\n",
       " 970,\n",
       " 971,\n",
       " 972,\n",
       " 973,\n",
       " 974,\n",
       " 975,\n",
       " 976,\n",
       " 977,\n",
       " 978,\n",
       " 979,\n",
       " 980,\n",
       " 981,\n",
       " 982,\n",
       " 983,\n",
       " 984,\n",
       " 985,\n",
       " 986,\n",
       " 987,\n",
       " 988,\n",
       " 989,\n",
       " 990,\n",
       " 991,\n",
       " 992,\n",
       " 993,\n",
       " 994,\n",
       " 995,\n",
       " 996,\n",
       " 997,\n",
       " 998,\n",
       " 999,\n",
       " ...}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_item_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(14909, 1.8628606),\n",
       " (1955, 1.8406407),\n",
       " (16909, 1.7795709),\n",
       " (21015, 1.7754593),\n",
       " (17611, 1.7729967),\n",
       " (8274, 1.75195),\n",
       " (20495, 1.7392932),\n",
       " (21263, 1.7232723),\n",
       " (5623, 1.7203313),\n",
       " (20069, 1.7122546),\n",
       " (12613, 1.7060773),\n",
       " (4208, 1.7003525),\n",
       " (20078, 1.683277),\n",
       " (20638, 1.6806306),\n",
       " (6215, 1.6736042),\n",
       " (953, 1.651202),\n",
       " (16038, 1.6423681),\n",
       " (14522, 1.6128026),\n",
       " (16727, 1.601887),\n",
       " (20172, 1.5859454),\n",
       " (1831, 1.5849069),\n",
       " (10734, 1.5782744),\n",
       " (11629, 1.5761318),\n",
       " (21583, 1.5742289),\n",
       " (18201, 1.5654683),\n",
       " (1981, 1.5629481),\n",
       " (8552, 1.556633),\n",
       " (445, 1.5542078),\n",
       " (1116, 1.5529963),\n",
       " (5323, 1.5479565),\n",
       " (20610, 1.5455303),\n",
       " (3243, 1.5426302),\n",
       " (10971, 1.5361725),\n",
       " (10330, 1.5350013),\n",
       " (11754, 1.5233514),\n",
       " (20219, 1.520942),\n",
       " (20073, 1.5188262),\n",
       " (15745, 1.5156312),\n",
       " (14564, 1.5056658),\n",
       " (135, 1.5047414),\n",
       " (6061, 1.503298),\n",
       " (21756, 1.5020897),\n",
       " (9235, 1.5018246),\n",
       " (6871, 1.4878578),\n",
       " (4875, 1.4869689),\n",
       " (9543, 1.4816445),\n",
       " (21623, 1.4801532),\n",
       " (15390, 1.4778388),\n",
       " (14679, 1.477642),\n",
       " (14444, 1.4718481),\n",
       " (15312, 1.4694368),\n",
       " (6180, 1.4603746),\n",
       " (12203, 1.4599557),\n",
       " (319, 1.4583042),\n",
       " (13798, 1.4571016),\n",
       " (14583, 1.4559591),\n",
       " (2639, 1.4540324),\n",
       " (18880, 1.4513447),\n",
       " (818, 1.448976),\n",
       " (6178, 1.4478507),\n",
       " (3918, 1.4420764),\n",
       " (10127, 1.4415185),\n",
       " (2189, 1.4413978),\n",
       " (2478, 1.4405098),\n",
       " (20402, 1.438144),\n",
       " (20161, 1.4376031),\n",
       " (2705, 1.4363976),\n",
       " (5167, 1.4354725),\n",
       " (22524, 1.4353967),\n",
       " (9411, 1.4352005),\n",
       " (17852, 1.4319046),\n",
       " (15837, 1.4317114),\n",
       " (2913, 1.4311302),\n",
       " (8862, 1.4300365),\n",
       " (16336, 1.4260612),\n",
       " (19936, 1.4230855),\n",
       " (8462, 1.4227233),\n",
       " (13933, 1.420691),\n",
       " (12622, 1.4173005),\n",
       " (1594, 1.4157679),\n",
       " (5562, 1.4135801),\n",
       " (17241, 1.4134147),\n",
       " (2356, 1.4082851),\n",
       " (12499, 1.4071696),\n",
       " (12115, 1.4045277),\n",
       " (685, 1.4039811),\n",
       " (21401, 1.4023789),\n",
       " (10182, 1.4015896),\n",
       " (13267, 1.4014983),\n",
       " (16451, 1.4013852),\n",
       " (20929, 1.4012353),\n",
       " (19272, 1.4005218),\n",
       " (11094, 1.3992418),\n",
       " (6119, 1.397002),\n",
       " (18626, 1.3942554),\n",
       " (21695, 1.3930825),\n",
       " (5636, 1.3923526),\n",
       " (5794, 1.3923416),\n",
       " (7569, 1.392128),\n",
       " (13676, 1.3920578),\n",
       " (18645, 1.391873),\n",
       " (21632, 1.3915057),\n",
       " (6404, 1.3907838),\n",
       " (6785, 1.3898575),\n",
       " (5605, 1.3896279),\n",
       " (13799, 1.3889424),\n",
       " (9133, 1.3855098),\n",
       " (12719, 1.3846025),\n",
       " (12190, 1.3825426),\n",
       " (20279, 1.3824672),\n",
       " (57, 1.3820171),\n",
       " (438, 1.3817763),\n",
       " (3377, 1.38159),\n",
       " (17270, 1.3809791),\n",
       " (11256, 1.3808217),\n",
       " (13428, 1.3798327),\n",
       " (18755, 1.3790622),\n",
       " (4524, 1.3713381),\n",
       " (9069, 1.3687699),\n",
       " (2475, 1.3677177),\n",
       " (6039, 1.3657105),\n",
       " (14520, 1.3652992),\n",
       " (16685, 1.3652639),\n",
       " (14114, 1.3651102),\n",
       " (21330, 1.3604505),\n",
       " (1418, 1.3596913),\n",
       " (8899, 1.3596177),\n",
       " (7776, 1.3593295),\n",
       " (15998, 1.3584262),\n",
       " (13437, 1.35731),\n",
       " (1233, 1.357157),\n",
       " (12431, 1.3545156),\n",
       " (1714, 1.3541679),\n",
       " (10472, 1.3541296),\n",
       " (19095, 1.3540497),\n",
       " (6369, 1.353733),\n",
       " (18178, 1.3526212),\n",
       " (1590, 1.3519598),\n",
       " (8806, 1.3508904),\n",
       " (17297, 1.348887),\n",
       " (11620, 1.3484799),\n",
       " (20212, 1.3484161),\n",
       " (3593, 1.346671),\n",
       " (9143, 1.3466651),\n",
       " (6971, 1.3462614),\n",
       " (7296, 1.3459136),\n",
       " (11872, 1.3457015),\n",
       " (13810, 1.3456762),\n",
       " (4722, 1.3434604),\n",
       " (20859, 1.3431361),\n",
       " (5873, 1.3430338),\n",
       " (1363, 1.3422545),\n",
       " (9232, 1.3418685),\n",
       " (9668, 1.3412638),\n",
       " (18685, 1.3406541),\n",
       " (17146, 1.3406523),\n",
       " (2569, 1.3389299),\n",
       " (1317, 1.338209),\n",
       " (22058, 1.3375902),\n",
       " (972, 1.3375211),\n",
       " (6617, 1.335403),\n",
       " (2185, 1.3345978),\n",
       " (1454, 1.3339236),\n",
       " (1169, 1.3312337),\n",
       " (15413, 1.3305084),\n",
       " (9210, 1.3298178),\n",
       " (7327, 1.3294611),\n",
       " (3621, 1.3282902),\n",
       " (9633, 1.3253667),\n",
       " (8023, 1.3249047),\n",
       " (17951, 1.3243105),\n",
       " (16578, 1.323764),\n",
       " (10499, 1.3233287),\n",
       " (4732, 1.3222646),\n",
       " (6616, 1.3218979),\n",
       " (21710, 1.3218417),\n",
       " (440, 1.3217765),\n",
       " (9028, 1.3213446),\n",
       " (4537, 1.3209424),\n",
       " (923, 1.3208718),\n",
       " (19887, 1.3208076),\n",
       " (12045, 1.320622),\n",
       " (7666, 1.3197758),\n",
       " (19503, 1.3195809),\n",
       " (19006, 1.3194567),\n",
       " (14210, 1.3184688),\n",
       " (16172, 1.3179402),\n",
       " (4697, 1.3178583),\n",
       " (8807, 1.3167447),\n",
       " (20403, 1.3163825),\n",
       " (18987, 1.3163054),\n",
       " (18606, 1.3162436),\n",
       " (4891, 1.315943),\n",
       " (6036, 1.3143654),\n",
       " (6250, 1.3143458),\n",
       " (307, 1.3130376),\n",
       " (6329, 1.3118455),\n",
       " (15721, 1.310415),\n",
       " (556, 1.3095181),\n",
       " (21000, 1.3082919),\n",
       " (16285, 1.3082519),\n",
       " (12195, 1.3076127),\n",
       " (20077, 1.3069665),\n",
       " (17758, 1.3061489),\n",
       " (4102, 1.3053689),\n",
       " (8682, 1.3046056),\n",
       " (12628, 1.3045105),\n",
       " (17092, 1.3042424),\n",
       " (17338, 1.3040869),\n",
       " (19096, 1.303141),\n",
       " (7045, 1.3018733),\n",
       " (1731, 1.3015176),\n",
       " (6511, 1.301372),\n",
       " (4853, 1.3002734),\n",
       " (14151, 1.2994738),\n",
       " (13276, 1.2993093),\n",
       " (14608, 1.2983354),\n",
       " (6458, 1.298291),\n",
       " (8013, 1.2970004),\n",
       " (1567, 1.2964022),\n",
       " (7717, 1.295024),\n",
       " (19535, 1.2947841),\n",
       " (140, 1.294331),\n",
       " (9837, 1.2940726),\n",
       " (1704, 1.2931473),\n",
       " (7451, 1.2907121),\n",
       " (5910, 1.2897246),\n",
       " (11282, 1.2895731),\n",
       " (3868, 1.2890679),\n",
       " (158, 1.288232),\n",
       " (16222, 1.2872703),\n",
       " (5222, 1.2869616),\n",
       " (712, 1.2868757),\n",
       " (10002, 1.2863867),\n",
       " (17172, 1.2857502),\n",
       " (21210, 1.2854738),\n",
       " (2895, 1.2846094),\n",
       " (15395, 1.2844366),\n",
       " (22439, 1.2838619),\n",
       " (18659, 1.283712),\n",
       " (23414, 1.2835222),\n",
       " (8475, 1.2832437),\n",
       " (3966, 1.2818115),\n",
       " (9270, 1.2817438),\n",
       " (20191, 1.2809644),\n",
       " (826, 1.2808614),\n",
       " (583, 1.2801039),\n",
       " (20148, 1.2798824),\n",
       " (18531, 1.2798347),\n",
       " (19376, 1.2794933),\n",
       " (17735, 1.2789142),\n",
       " (12853, 1.2784016),\n",
       " (6489, 1.2779801),\n",
       " (3869, 1.2772245),\n",
       " (14384, 1.2762725),\n",
       " (12971, 1.2754922),\n",
       " (969, 1.2750494),\n",
       " (6510, 1.2748348),\n",
       " (4363, 1.2736063),\n",
       " (6802, 1.2733188),\n",
       " (1703, 1.2730472),\n",
       " (5045, 1.2724693),\n",
       " (3451, 1.2721418),\n",
       " (22687, 1.2717459),\n",
       " (18886, 1.2713773),\n",
       " (18357, 1.2710392),\n",
       " (3131, 1.2703309),\n",
       " (20482, 1.2701533),\n",
       " (18929, 1.2701287),\n",
       " (19565, 1.2697408),\n",
       " (2011, 1.2696409),\n",
       " (419, 1.269138),\n",
       " (16371, 1.2686312),\n",
       " (16857, 1.267943),\n",
       " (21722, 1.2678344),\n",
       " (21033, 1.2671049),\n",
       " (15152, 1.2645547),\n",
       " (6131, 1.264267),\n",
       " (14014, 1.2641919),\n",
       " (12328, 1.2639865),\n",
       " (5005, 1.2638471),\n",
       " (21816, 1.2628152),\n",
       " (11177, 1.262815),\n",
       " (6003, 1.2626712),\n",
       " (17190, 1.2617302),\n",
       " (20170, 1.2616433),\n",
       " (4650, 1.261633),\n",
       " (15836, 1.2604877),\n",
       " (5182, 1.260323),\n",
       " (16101, 1.2597219),\n",
       " (5822, 1.2590168),\n",
       " (16592, 1.2589071),\n",
       " (16488, 1.2577949),\n",
       " (23111, 1.2572882),\n",
       " (13306, 1.2566855),\n",
       " (5474, 1.2564313),\n",
       " (12886, 1.25621),\n",
       " (14443, 1.2561092),\n",
       " (16787, 1.2548106),\n",
       " (476, 1.2546524),\n",
       " (17889, 1.2546425),\n",
       " (21896, 1.2545598),\n",
       " (12756, 1.2539723),\n",
       " (5992, 1.2533162),\n",
       " (18342, 1.2530665),\n",
       " (20989, 1.2530117),\n",
       " (12164, 1.2523903),\n",
       " (14041, 1.2521073),\n",
       " (12353, 1.2517965),\n",
       " (17767, 1.251754),\n",
       " (4425, 1.2515616),\n",
       " (22584, 1.2513988),\n",
       " (18661, 1.2505763),\n",
       " (14626, 1.2505035),\n",
       " (14303, 1.2504476),\n",
       " (20308, 1.2500656),\n",
       " (19678, 1.2495816),\n",
       " (18394, 1.2491899),\n",
       " (18792, 1.2491775),\n",
       " (6133, 1.2478199),\n",
       " (3045, 1.2472749),\n",
       " (21958, 1.2470636),\n",
       " (15662, 1.2468344),\n",
       " (3245, 1.2442153),\n",
       " (8266, 1.2439563),\n",
       " (16715, 1.2439358),\n",
       " (19072, 1.2438369),\n",
       " (16979, 1.2426299),\n",
       " (5969, 1.2424204),\n",
       " (174, 1.241745),\n",
       " (11933, 1.2407832),\n",
       " (2099, 1.240776),\n",
       " (4969, 1.2407241),\n",
       " (5707, 1.2402935),\n",
       " (5365, 1.239896),\n",
       " (3577, 1.2397859),\n",
       " (23399, 1.2395873),\n",
       " (19276, 1.2389193),\n",
       " (2132, 1.2378527),\n",
       " (18720, 1.2368867),\n",
       " (13893, 1.2368212),\n",
       " (6588, 1.2367017),\n",
       " (2227, 1.2365175),\n",
       " (19013, 1.2359023),\n",
       " (7450, 1.235667),\n",
       " (19851, 1.2354194),\n",
       " (7374, 1.2352264),\n",
       " (16554, 1.2350545),\n",
       " (5721, 1.2350484),\n",
       " (16216, 1.2347491),\n",
       " (5065, 1.2346916),\n",
       " (20010, 1.2345133),\n",
       " (17830, 1.2340328),\n",
       " (20176, 1.2334336),\n",
       " (17368, 1.2326294),\n",
       " (2328, 1.2325845),\n",
       " (5089, 1.2322296),\n",
       " (14188, 1.2320735),\n",
       " (4874, 1.2318481),\n",
       " (5732, 1.2315493),\n",
       " (9161, 1.2313905),\n",
       " (2963, 1.2313818),\n",
       " (14937, 1.2307589),\n",
       " (4498, 1.2307402),\n",
       " (4646, 1.2301288),\n",
       " (16684, 1.2301159),\n",
       " (2481, 1.2297633),\n",
       " (16218, 1.2293932),\n",
       " (10819, 1.2292926),\n",
       " (18159, 1.228974),\n",
       " (15381, 1.2284309),\n",
       " (3733, 1.2284218),\n",
       " (17948, 1.2283635),\n",
       " (2252, 1.2280456),\n",
       " (9548, 1.2274761),\n",
       " (18060, 1.2271808),\n",
       " (21104, 1.2271694),\n",
       " (317, 1.2267324),\n",
       " (2201, 1.2264723),\n",
       " (4390, 1.2257911),\n",
       " (1427, 1.2254162),\n",
       " (9126, 1.2250876),\n",
       " (4064, 1.2250259),\n",
       " (13984, 1.2237072),\n",
       " (1832, 1.2233064),\n",
       " (7337, 1.2232065),\n",
       " (8964, 1.2230539),\n",
       " (2434, 1.2221895),\n",
       " (952, 1.2217983),\n",
       " (16668, 1.2217509),\n",
       " (3941, 1.221694),\n",
       " (11943, 1.2188511),\n",
       " (15553, 1.2176573),\n",
       " (6817, 1.2176061),\n",
       " (9313, 1.2171359),\n",
       " (1665, 1.2162127),\n",
       " (1488, 1.215477),\n",
       " (1844, 1.214874),\n",
       " (9369, 1.2147846),\n",
       " (16750, 1.2141118),\n",
       " (7610, 1.2133031),\n",
       " (7234, 1.2131263),\n",
       " (10808, 1.2130082),\n",
       " (19566, 1.2129688),\n",
       " (8011, 1.2128367),\n",
       " (4814, 1.2127987),\n",
       " (14728, 1.2126863),\n",
       " (19992, 1.2123665),\n",
       " (19616, 1.212261),\n",
       " (4726, 1.2112905),\n",
       " (7738, 1.2103286),\n",
       " (13700, 1.2098116),\n",
       " (8873, 1.209342),\n",
       " (21524, 1.2092803),\n",
       " (18216, 1.2089823),\n",
       " (21466, 1.2086623),\n",
       " (13264, 1.2084649),\n",
       " (21227, 1.2084504),\n",
       " (14448, 1.2084135),\n",
       " (1309, 1.2074308),\n",
       " (15625, 1.2069976),\n",
       " (7214, 1.2069614),\n",
       " (7662, 1.2056706),\n",
       " (15687, 1.2050353),\n",
       " (11843, 1.2047204),\n",
       " (15267, 1.2043146),\n",
       " (13673, 1.2042699),\n",
       " (7609, 1.2042344),\n",
       " (21026, 1.2040082),\n",
       " (21159, 1.2033131),\n",
       " (5875, 1.2032216),\n",
       " (173, 1.2031472),\n",
       " (13048, 1.2024374),\n",
       " (18718, 1.2021189),\n",
       " (15987, 1.2019694),\n",
       " (1470, 1.2017822),\n",
       " (18870, 1.2015221),\n",
       " (3128, 1.2014338),\n",
       " (18395, 1.2013226),\n",
       " (11553, 1.2006316),\n",
       " (3280, 1.2006253),\n",
       " (7046, 1.2006153),\n",
       " (12033, 1.2003691),\n",
       " (14977, 1.2001799),\n",
       " (10672, 1.2001777),\n",
       " (3740, 1.1989998),\n",
       " (16965, 1.1987319),\n",
       " (11400, 1.1985104),\n",
       " (2278, 1.1978838),\n",
       " (2863, 1.197707),\n",
       " (6174, 1.1973188),\n",
       " (18131, 1.1968329),\n",
       " (11275, 1.1961958),\n",
       " (22714, 1.1960597),\n",
       " (19207, 1.1958001),\n",
       " (7728, 1.1953431),\n",
       " (11124, 1.1952772),\n",
       " (137, 1.194536),\n",
       " (5710, 1.1942539),\n",
       " (778, 1.1941955),\n",
       " (18114, 1.1935534),\n",
       " (2983, 1.1935009),\n",
       " (3910, 1.1933432),\n",
       " (2351, 1.1932311),\n",
       " (17535, 1.1932197),\n",
       " (20952, 1.1931238),\n",
       " (7551, 1.1927156),\n",
       " (20540, 1.1924039),\n",
       " (6744, 1.1921021),\n",
       " (691, 1.1917915),\n",
       " (21157, 1.1913855),\n",
       " (15121, 1.1909363),\n",
       " (18170, 1.1896877),\n",
       " (2329, 1.1894118),\n",
       " (5006, 1.1880164),\n",
       " (4289, 1.1878606),\n",
       " (612, 1.1877694),\n",
       " (4510, 1.1877147),\n",
       " (1723, 1.1876589),\n",
       " (2893, 1.1876117),\n",
       " (124, 1.1875671),\n",
       " (11511, 1.1868056),\n",
       " (11337, 1.1865095),\n",
       " (1640, 1.186167),\n",
       " (20016, 1.1855202),\n",
       " (4654, 1.185278),\n",
       " (14184, 1.1851351),\n",
       " (3811, 1.1850587),\n",
       " (13415, 1.1843479),\n",
       " (16391, 1.1841905),\n",
       " (2274, 1.1840532),\n",
       " (18370, 1.1835055),\n",
       " (271, 1.1831052),\n",
       " (13681, 1.1823604),\n",
       " (22355, 1.1818662),\n",
       " (5445, 1.1818478),\n",
       " (2097, 1.1817315),\n",
       " (12273, 1.1817043),\n",
       " (1379, 1.1816883),\n",
       " (16845, 1.1812978),\n",
       " (13759, 1.1810236),\n",
       " (15897, 1.1799071),\n",
       " (14721, 1.1798146),\n",
       " (4011, 1.1787694),\n",
       " (15410, 1.1787643),\n",
       " (2292, 1.1787541),\n",
       " (5538, 1.178648),\n",
       " (15099, 1.1780678),\n",
       " (2920, 1.1779402),\n",
       " (280, 1.1775451),\n",
       " (3992, 1.1774873),\n",
       " (11706, 1.17698),\n",
       " (522, 1.17693),\n",
       " (6671, 1.176753),\n",
       " (11194, 1.176648),\n",
       " (16642, 1.176213),\n",
       " (7568, 1.17607),\n",
       " (19454, 1.1759807),\n",
       " (2096, 1.1758776),\n",
       " (15867, 1.1757298),\n",
       " (3531, 1.1755933),\n",
       " (21707, 1.1754379),\n",
       " (18719, 1.1749871),\n",
       " (19497, 1.1743586),\n",
       " (13453, 1.1738333),\n",
       " (13800, 1.1737835),\n",
       " (711, 1.1733612),\n",
       " (3890, 1.1729194),\n",
       " (3622, 1.1728504),\n",
       " (4819, 1.172441),\n",
       " (1118, 1.1723405),\n",
       " (12723, 1.1721783),\n",
       " (15178, 1.1719822),\n",
       " (6456, 1.17152),\n",
       " (8036, 1.171301),\n",
       " (21193, 1.1711892),\n",
       " (17356, 1.1710145),\n",
       " (1245, 1.1708412),\n",
       " (20217, 1.1708283),\n",
       " (3770, 1.1707494),\n",
       " (3042, 1.170567),\n",
       " (5526, 1.1704413),\n",
       " (357, 1.1703985),\n",
       " (3765, 1.1703078),\n",
       " (16564, 1.1702063),\n",
       " (19327, 1.1698675),\n",
       " (1620, 1.1698079),\n",
       " (14449, 1.169636),\n",
       " (15601, 1.1695046),\n",
       " (13953, 1.1691983),\n",
       " (17770, 1.1676201),\n",
       " (3454, 1.1672035),\n",
       " (2330, 1.1670582),\n",
       " (9836, 1.1670504),\n",
       " (3130, 1.1670237),\n",
       " (6508, 1.1669812),\n",
       " (10025, 1.166797),\n",
       " (15215, 1.1663971),\n",
       " (3084, 1.1662713),\n",
       " (17618, 1.1659837),\n",
       " (9958, 1.1655072),\n",
       " (1591, 1.1654024),\n",
       " (1629, 1.1649833),\n",
       " (9355, 1.1645689),\n",
       " (718, 1.1636477),\n",
       " (3270, 1.1636403),\n",
       " (1234, 1.1635303),\n",
       " (3677, 1.1634455),\n",
       " (7688, 1.1633995),\n",
       " (5621, 1.163266),\n",
       " (1452, 1.16323),\n",
       " (9730, 1.1624725),\n",
       " (6733, 1.1623932),\n",
       " (4529, 1.1621463),\n",
       " (11773, 1.1619995),\n",
       " (17322, 1.1619875),\n",
       " (20537, 1.1618137),\n",
       " (21032, 1.1616371),\n",
       " (7324, 1.1614399),\n",
       " (17222, 1.1611562),\n",
       " (8199, 1.1611032),\n",
       " (22483, 1.1607364),\n",
       " (12387, 1.1604366),\n",
       " (2978, 1.1603982),\n",
       " (847, 1.160166),\n",
       " (1310, 1.1598701),\n",
       " (986, 1.1593918),\n",
       " (1115, 1.1589913),\n",
       " (5024, 1.1584812),\n",
       " (17877, 1.158217),\n",
       " (19302, 1.1574969),\n",
       " (13274, 1.1574112),\n",
       " (14829, 1.1574092),\n",
       " (2694, 1.1572881),\n",
       " (2627, 1.1569228),\n",
       " (1077, 1.1565762),\n",
       " (16044, 1.1561574),\n",
       " (19698, 1.1561424),\n",
       " (14153, 1.1561215),\n",
       " (13397, 1.1553931),\n",
       " (21024, 1.1552751),\n",
       " (20184, 1.1548669),\n",
       " (6832, 1.1543168),\n",
       " (6213, 1.1542096),\n",
       " (4444, 1.1538732),\n",
       " (516, 1.1531317),\n",
       " (17739, 1.1527035),\n",
       " (20425, 1.1525867),\n",
       " (3268, 1.1514969),\n",
       " (23269, 1.1513386),\n",
       " (2785, 1.1513155),\n",
       " (23373, 1.1511598),\n",
       " (23122, 1.150752),\n",
       " (7328, 1.1507432),\n",
       " (6004, 1.1503634),\n",
       " (2331, 1.1503454),\n",
       " (14656, 1.150303),\n",
       " (17078, 1.1501918),\n",
       " (12232, 1.1499674),\n",
       " (13724, 1.1496613),\n",
       " (13653, 1.1495962),\n",
       " (17203, 1.1492943),\n",
       " (17965, 1.1491745),\n",
       " (21489, 1.1485089),\n",
       " (20211, 1.1483024),\n",
       " (20062, 1.147685),\n",
       " (3840, 1.1474237),\n",
       " (17195, 1.1472344),\n",
       " (16767, 1.1470369),\n",
       " (15941, 1.1466999),\n",
       " (14045, 1.1460645),\n",
       " (353, 1.1455929),\n",
       " (21729, 1.1453743),\n",
       " (13938, 1.1450322),\n",
       " (3439, 1.1448671),\n",
       " (17401, 1.1445341),\n",
       " (5862, 1.1443661),\n",
       " (9822, 1.1439548),\n",
       " (2192, 1.1439185),\n",
       " (11082, 1.143547),\n",
       " (21481, 1.1435168),\n",
       " (9750, 1.1430008),\n",
       " (5447, 1.1427944),\n",
       " (774, 1.1424918),\n",
       " (3146, 1.1423645),\n",
       " (20348, 1.1420639),\n",
       " (856, 1.1420599),\n",
       " (14601, 1.1415296),\n",
       " (2772, 1.1414721),\n",
       " (21182, 1.1411413),\n",
       " (9111, 1.1407269),\n",
       " (5675, 1.1404161),\n",
       " (20612, 1.1399393),\n",
       " (3899, 1.1399214),\n",
       " (13335, 1.1391189),\n",
       " (6834, 1.1390963),\n",
       " (14720, 1.1387825),\n",
       " (477, 1.1387204),\n",
       " (22830, 1.1385546),\n",
       " (8219, 1.1381804),\n",
       " (17862, 1.1381252),\n",
       " (16990, 1.1377028),\n",
       " (479, 1.1376605),\n",
       " (14866, 1.1371249),\n",
       " (12760, 1.1369581),\n",
       " (5063, 1.136887),\n",
       " (18740, 1.1365716),\n",
       " (15630, 1.136488),\n",
       " (23302, 1.135767),\n",
       " (4349, 1.1355634),\n",
       " (7067, 1.13522),\n",
       " (14237, 1.1347332),\n",
       " (2697, 1.1346247),\n",
       " (17167, 1.1345506),\n",
       " (1494, 1.1341823),\n",
       " (2564, 1.1335745),\n",
       " (16586, 1.1331915),\n",
       " (14010, 1.132983),\n",
       " (9770, 1.1329597),\n",
       " (6513, 1.1329426),\n",
       " (3698, 1.1328996),\n",
       " (16526, 1.1328459),\n",
       " (13588, 1.1327276),\n",
       " (1194, 1.1325457),\n",
       " (300, 1.1324048),\n",
       " (22413, 1.1323061),\n",
       " (1715, 1.1322898),\n",
       " (16928, 1.1322498),\n",
       " (6118, 1.1315708),\n",
       " (1628, 1.1313442),\n",
       " (15386, 1.1312928),\n",
       " (16479, 1.1311876),\n",
       " (11630, 1.1310759),\n",
       " (8963, 1.1310669),\n",
       " (200, 1.1306846),\n",
       " (8242, 1.1304893),\n",
       " (18745, 1.1300513),\n",
       " (295, 1.1297445),\n",
       " (5766, 1.1296866),\n",
       " (4362, 1.1296636),\n",
       " (2861, 1.1287585),\n",
       " (53, 1.1287425),\n",
       " (17281, 1.1282942),\n",
       " (4893, 1.1279953),\n",
       " (3123, 1.1264982),\n",
       " (33, 1.1262832),\n",
       " (4964, 1.1258881),\n",
       " (4898, 1.1255325),\n",
       " (409, 1.1254404),\n",
       " (17432, 1.125317),\n",
       " (298, 1.1253085),\n",
       " (984, 1.1251256),\n",
       " (9735, 1.1247222),\n",
       " (9110, 1.1247112),\n",
       " (14592, 1.124562),\n",
       " (11084, 1.1238308),\n",
       " (13016, 1.1237187),\n",
       " (9327, 1.1236336),\n",
       " (22521, 1.1230956),\n",
       " (17445, 1.1226776),\n",
       " (515, 1.1225966),\n",
       " (4655, 1.1224976),\n",
       " (14204, 1.1219567),\n",
       " (13309, 1.1219305),\n",
       " (4852, 1.1218493),\n",
       " (22290, 1.1214468),\n",
       " (2482, 1.1212499),\n",
       " (13042, 1.1212103),\n",
       " (5186, 1.1211971),\n",
       " (4438, 1.1209817),\n",
       " (371, 1.1209192),\n",
       " (21198, 1.1207161),\n",
       " (1480, 1.1206579),\n",
       " (4038, 1.1203504),\n",
       " (18664, 1.1201572),\n",
       " (9544, 1.1198095),\n",
       " (12265, 1.1197641),\n",
       " (14970, 1.1195333),\n",
       " (21034, 1.1194005),\n",
       " (10674, 1.1185521),\n",
       " (558, 1.1184237),\n",
       " (11956, 1.1178432),\n",
       " (2333, 1.117617),\n",
       " (21211, 1.117493),\n",
       " (12673, 1.1174474),\n",
       " (10516, 1.1173797),\n",
       " (10086, 1.1169991),\n",
       " (3588, 1.1168027),\n",
       " (713, 1.1167728),\n",
       " (15877, 1.1167091),\n",
       " (4274, 1.1164407),\n",
       " (18240, 1.1162672),\n",
       " (17059, 1.115829),\n",
       " (4254, 1.1156846),\n",
       " (3261, 1.1156598),\n",
       " (2308, 1.11501),\n",
       " (19099, 1.1149029),\n",
       " (1136, 1.11483),\n",
       " (21518, 1.1146505),\n",
       " (738, 1.114402),\n",
       " (5064, 1.1142051),\n",
       " (10596, 1.1139004),\n",
       " (557, 1.1135482),\n",
       " (17936, 1.1134536),\n",
       " (15765, 1.1132971),\n",
       " (6934, 1.1131485),\n",
       " (16181, 1.1130042),\n",
       " (2186, 1.1126392),\n",
       " (20475, 1.1122062),\n",
       " (14785, 1.1120318),\n",
       " (6521, 1.1117735),\n",
       " (6472, 1.1117141),\n",
       " (17040, 1.1115417),\n",
       " (10731, 1.1112392),\n",
       " (1479, 1.1107627),\n",
       " (178, 1.1102935),\n",
       " (17858, 1.1102339),\n",
       " (8677, 1.1102171),\n",
       " (22415, 1.1101611),\n",
       " (6231, 1.1099632),\n",
       " (311, 1.1097617),\n",
       " (4501, 1.1095408),\n",
       " (5493, 1.109435),\n",
       " (3496, 1.1090595),\n",
       " (12399, 1.1089952),\n",
       " (22075, 1.1089578),\n",
       " (3148, 1.1088047),\n",
       " (16912, 1.1085212),\n",
       " (2419, 1.1084766),\n",
       " (1664, 1.1079237),\n",
       " (16647, 1.1078503),\n",
       " (14158, 1.1077931),\n",
       " (13994, 1.1073854),\n",
       " (15279, 1.1073556),\n",
       " (13725, 1.1073527),\n",
       " (6914, 1.1072204),\n",
       " (8217, 1.1068358),\n",
       " (8018, 1.1066046),\n",
       " (13012, 1.1060073),\n",
       " (13715, 1.1050478),\n",
       " (3305, 1.105016),\n",
       " (3282, 1.1047041),\n",
       " (13555, 1.1045864),\n",
       " (8247, 1.1044154),\n",
       " (1031, 1.1040568),\n",
       " (1378, 1.1040521),\n",
       " (7242, 1.103999),\n",
       " (23031, 1.1033151),\n",
       " (1399, 1.1027427),\n",
       " (3457, 1.1027393),\n",
       " (9236, 1.1024015),\n",
       " (13027, 1.1020627),\n",
       " (14338, 1.1017857),\n",
       " (12136, 1.1017082),\n",
       " (11242, 1.1015632),\n",
       " (12401, 1.1009457),\n",
       " (6502, 1.1008043),\n",
       " (4434, 1.1007563),\n",
       " (1131, 1.1006474),\n",
       " (2134, 1.1005874),\n",
       " (7822, 1.1005634),\n",
       " (18991, 1.1003233),\n",
       " (20350, 1.1001174),\n",
       " (17968, 1.0998564),\n",
       " (18873, 1.0997581),\n",
       " (12773, 1.0996192),\n",
       " (21063, 1.0991809),\n",
       " (4601, 1.0990617),\n",
       " (13011, 1.0989922),\n",
       " (6938, 1.0988998),\n",
       " (5870, 1.0982095),\n",
       " (18874, 1.0980859),\n",
       " (8043, 1.0979923),\n",
       " (19938, 1.097723),\n",
       " (3310, 1.0975631),\n",
       " (692, 1.0974691),\n",
       " (7958, 1.0974331),\n",
       " (18200, 1.0964677),\n",
       " (918, 1.0963157),\n",
       " (6774, 1.0958339),\n",
       " (2985, 1.0955007),\n",
       " (8464, 1.0953124),\n",
       " (2751, 1.0953078),\n",
       " (18493, 1.0950935),\n",
       " (3447, 1.0948875),\n",
       " (21082, 1.094451),\n",
       " (6845, 1.0941472),\n",
       " (761, 1.0939953),\n",
       " (12839, 1.0939608),\n",
       " (19082, 1.093857),\n",
       " (12744, 1.0938424),\n",
       " (6740, 1.0938306),\n",
       " (6392, 1.0938249),\n",
       " (10860, 1.0934955),\n",
       " (9476, 1.0934496),\n",
       " (4661, 1.0932921),\n",
       " (12927, 1.0930315),\n",
       " (20958, 1.0929711),\n",
       " (29, 1.0927764),\n",
       " (2288, 1.0925971),\n",
       " (2962, 1.0925922),\n",
       " (22418, 1.0925035),\n",
       " (20736, 1.0922868),\n",
       " (13028, 1.0916431),\n",
       " (13014, 1.0915651),\n",
       " (21643, 1.0911859),\n",
       " (13991, 1.0909514),\n",
       " (19793, 1.090944),\n",
       " (3418, 1.0905604),\n",
       " (16693, 1.0899149),\n",
       " (22689, 1.0893935),\n",
       " (6229, 1.0892553),\n",
       " (6358, 1.0889232),\n",
       " (4515, 1.0889146),\n",
       " (16637, 1.0886266),\n",
       " (6573, 1.0873026),\n",
       " (3712, 1.0871966),\n",
       " (13894, 1.08709),\n",
       " (6543, 1.0870686),\n",
       " (14051, 1.0868804),\n",
       " (12694, 1.0868192),\n",
       " (7386, 1.0868118),\n",
       " (19515, 1.0866785),\n",
       " (9545, 1.0864818),\n",
       " (7755, 1.0864604),\n",
       " (758, 1.0864482),\n",
       " (2105, 1.0860057),\n",
       " (4209, 1.0859483),\n",
       " (6806, 1.0858496),\n",
       " (270, 1.0855939),\n",
       " (2102, 1.0855352),\n",
       " (7570, 1.0850737),\n",
       " (14361, 1.0848423),\n",
       " (17854, 1.0848148),\n",
       " (6784, 1.0847528),\n",
       " (143, 1.0839844),\n",
       " (1478, 1.0839695),\n",
       " (17929, 1.0837698),\n",
       " (114, 1.0836537),\n",
       " (12798, 1.0835361),\n",
       " (18603, 1.0835326),\n",
       " (7612, 1.0833269),\n",
       " (11197, 1.083185),\n",
       " (3596, 1.0830214),\n",
       " (15954, 1.0829926),\n",
       " (13596, 1.0829766),\n",
       " (146, 1.082855),\n",
       " (4738, 1.0828445),\n",
       " (6101, 1.0828357),\n",
       " (19446, 1.0827848),\n",
       " (12556, 1.0823926),\n",
       " (11056, 1.0823514),\n",
       " (1124, 1.0821087),\n",
       " (6939, 1.0820072),\n",
       " (4372, 1.0819058),\n",
       " (7232, 1.0814242),\n",
       " (16735, 1.0803677),\n",
       " (9338, 1.0802925),\n",
       " (4839, 1.0800829),\n",
       " (9344, 1.0795776),\n",
       " (8990, 1.0793498),\n",
       " (7005, 1.0789236),\n",
       " (7099, 1.0789157),\n",
       " (14071, 1.078873),\n",
       " (21449, 1.0779659),\n",
       " (16533, 1.0779605),\n",
       " (1210, 1.077901),\n",
       " (19749, 1.0776732),\n",
       " (2070, 1.0776608),\n",
       " (4815, 1.0763049),\n",
       " (21592, 1.0761654),\n",
       " (19579, 1.0758967),\n",
       " (9477, 1.0758615),\n",
       " (8314, 1.0756857),\n",
       " (22139, 1.0748806),\n",
       " (3086, 1.0748785),\n",
       " (17312, 1.0748627),\n",
       " (7068, 1.0747097),\n",
       " (17745, 1.0741258),\n",
       " (16583, 1.0740688),\n",
       " (20827, 1.0738573),\n",
       " (21993, 1.073772),\n",
       " (848, 1.0734974),\n",
       " (12037, 1.0734746),\n",
       " (14013, 1.0734711),\n",
       " (23074, 1.0733318),\n",
       " (4439, 1.073143),\n",
       " (8150, 1.0727627),\n",
       " (7562, 1.0726438),\n",
       " (6337, 1.0725051),\n",
       " (15684, 1.0725),\n",
       " (19548, 1.0724589),\n",
       " (11491, 1.072333),\n",
       " (19206, 1.0716734),\n",
       " (13303, 1.0715668),\n",
       " (21353, 1.0713255),\n",
       " (3864, 1.0713027),\n",
       " (7163, 1.0709845),\n",
       " (5746, 1.0707021),\n",
       " (1431, 1.0705891),\n",
       " (3732, 1.0696007),\n",
       " (18035, 1.06947),\n",
       " (14768, 1.0694609),\n",
       " (1186, 1.0694219),\n",
       " (4062, 1.0693977),\n",
       " (5071, 1.0693567),\n",
       " (16137, 1.0692601),\n",
       " (4186, 1.0691109),\n",
       " (20011, 1.0688753),\n",
       " (10673, 1.0683112),\n",
       " (1246, 1.0683082),\n",
       " (8474, 1.0682107),\n",
       " (2254, 1.0681591),\n",
       " (971, 1.0679443),\n",
       " (269, 1.0678751),\n",
       " (19537, 1.0678232),\n",
       " (13179, 1.0675216),\n",
       " (9312, 1.067269),\n",
       " (15459, 1.0671928),\n",
       " (1474, 1.0670172),\n",
       " (14637, 1.0669272),\n",
       " (5896, 1.0668671),\n",
       " (19896, 1.0665405),\n",
       " (6793, 1.0665311),\n",
       " (22555, 1.0663862),\n",
       " (14782, 1.0662696),\n",
       " (18172, 1.0661736),\n",
       " (359, 1.066139),\n",
       " (6672, 1.0656126),\n",
       " (4968, 1.0654627),\n",
       " (283, 1.0654218),\n",
       " (6236, 1.0653801),\n",
       " (518, 1.0652412),\n",
       " (7820, 1.0651321),\n",
       " (7069, 1.0650314),\n",
       " (397, 1.0649935),\n",
       " (12427, 1.0648967),\n",
       " (14582, 1.0648756),\n",
       " (9957, 1.0647635),\n",
       " (12944, 1.0643904),\n",
       " ...]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = tf.Variable([[1, 2, 3], [4, 5, 6]], dtype=tf.float32)\n",
    "t2 = tf.Variable([[7, 8, 9], [10, 11, 12]], dtype=tf.float32)\n",
    "t3 = tf.concat([t1, t2], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    out = tf.reduce_sum(t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tape.gradient(t3, [t1, t2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
