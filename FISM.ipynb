{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras import Model\n",
    "import pickle\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import os\n",
    "import random\n",
    "from time import time\n",
    "import itertools\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_visible_devices(gpus[1:3], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "\n",
    "    def __init__(self, train_file, test_file, negative_sample=3, batch_size=64):\n",
    "        self.train_data = pd.read_csv(train_file)\n",
    "        self.test_data = pd.read_csv(test_file)\n",
    "        self.num_users = self.train_data['user_id'].max() + 1\n",
    "        self.num_items = self.train_data['item_id'].max() + 1\n",
    "        self.negative_sample = negative_sample\n",
    "        self.batch_size = batch_size\n",
    "        self.user_rated_items = self.get_user_rated_items()\n",
    "        self.num_batch = -1\n",
    "        self.all_train_data = None\n",
    "        self.test_users = self.get_test_user()\n",
    "\n",
    "    def get_test_user(self):\n",
    "        test_user_dict = dict()\n",
    "        for user_id, item_id in self.test_data[['user_id', 'item_id']].values:\n",
    "            if not test_user_dict.__contains__(user_id):\n",
    "                test_user_dict[user_id] = []\n",
    "            test_user_dict[user_id].append(item_id)\n",
    "        return test_user_dict\n",
    "\n",
    "    def get_user_rated_items(self):\n",
    "        rated_data = self.get_rated_data()\n",
    "        user_rated_items = dict()\n",
    "        for user_id, item_id, rate in rated_data:\n",
    "            if not user_rated_items.__contains__(user_id):\n",
    "                user_rated_items[user_id] = set()\n",
    "            user_rated_items[user_id].add(item_id)\n",
    "        return user_rated_items\n",
    "\n",
    "    def prepare_train_data(self):\n",
    "        rated_data = self.get_rated_data()\n",
    "        np.random.shuffle(rated_data)\n",
    "        self.all_train_data = self.negative_sampling(rated_data)\n",
    "        self.num_batch = self.all_train_data[0].__len__()//self.batch_size\n",
    "\n",
    "    def get_batch(self, i):\n",
    "        user_ids, item_ids, labels, ratings = self.all_train_data\n",
    "        batch_user_descriptions = []\n",
    "        batch_item_ids = item_ids[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "        batch_user_ids = user_ids[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "        batch_num_items = []\n",
    "        batch_labels = labels[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "        batch_ratings = ratings[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "        mask = self.num_items\n",
    "        for j in range(self.batch_size):\n",
    "            idx = i * self.batch_size + j\n",
    "            user_id = user_ids[idx]\n",
    "            item_id = item_ids[idx]\n",
    "            rated_items = self.user_rated_items[user_id].copy()\n",
    "            user_description = self.get_user_description(rated_items, item_id)\n",
    "            # user_description = []\n",
    "            batch_user_descriptions.append(user_description)\n",
    "            batch_num_items.append(user_description.__len__())\n",
    "        max_user_des = max(batch_num_items)\n",
    "        # max_user_des = 10\n",
    "        batch_user_descriptions = self.padding_user_description(batch_user_descriptions, mask, max_user_des)\n",
    "        return (batch_user_descriptions,\n",
    "                np.array(batch_user_ids, dtype=np.int32),\n",
    "                np.array(batch_item_ids, dtype=np.int32),\n",
    "                np.array(batch_num_items, dtype=np.float32),\n",
    "                np.array(batch_labels, dtype=np.float32),\n",
    "                np.array(batch_ratings, dtype=np.float32)\n",
    "                )\n",
    "\n",
    "    def generate_train_data(self):\n",
    "        rated_data = self.get_rated_data()\n",
    "        #         print('done load data')\n",
    "        np.random.shuffle(rated_data)\n",
    "        all_train_data = self.negative_sampling(rated_data)\n",
    "        all_batch_data = self.get_all_batch_data(all_train_data, self.user_rated_items)\n",
    "        return all_batch_data\n",
    "\n",
    "    def get_rated_data(self):\n",
    "        return [(user_id, item_id, rate) for user_id, item_id, rate in self.train_data[['user_id', 'item_id', 'rating']].values]\n",
    "\n",
    "    def negative_sampling(self, rated_data):\n",
    "        user_ids = []\n",
    "        item_ids = []\n",
    "        labels = []\n",
    "        ratings = []\n",
    "        set_rated = {(user_id, item_id) for user_id, item_id, rating in rated_data}\n",
    "        for user_id, item_id, rating in rated_data:\n",
    "            user_ids.append(user_id)\n",
    "            item_ids.append(item_id)\n",
    "            labels.append(1)\n",
    "            ratings.append(rating)\n",
    "            for j in range(self.negative_sample):\n",
    "                random_item = np.random.randint(self.num_items)\n",
    "                while set_rated.__contains__((user_id, random_item)):\n",
    "                    random_item = np.random.randint(self.num_items)\n",
    "                user_ids.append(user_id)\n",
    "                item_ids.append(random_item)\n",
    "                labels.append(0)\n",
    "                ratings.append(0)\n",
    "        return user_ids, item_ids, labels, ratings\n",
    "\n",
    "    def get_all_batch_data(self, all_train_data, user_rated_items):\n",
    "        user_ids, item_ids, labels, ratings = all_train_data\n",
    "        mask = self.num_items\n",
    "        num_iter = user_ids.__len__() // self.batch_size\n",
    "        batch_data = []\n",
    "        for i in tqdm(range(num_iter)):\n",
    "            batch_user_descriptions = []\n",
    "            batch_item_ids = item_ids[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "            batch_user_ids = user_ids[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "            batch_num_items = []\n",
    "            batch_labels = labels[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "            batch_ratings = ratings[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "\n",
    "            for j in range(self.batch_size):\n",
    "                idx = i * self.batch_size + j\n",
    "                user_id = user_ids[idx]\n",
    "                item_id = item_ids[idx]\n",
    "                # label = labels[idx][0]\n",
    "                # rating = labels[idx][1]\n",
    "                rated_items = user_rated_items[user_id].copy()\n",
    "                batch_user_descriptions.append(self.get_user_description(rated_items, item_id))\n",
    "                batch_num_items.append(batch_user_descriptions[-1].__len__())\n",
    "                # batch_user_ids.append(user_id)\n",
    "                # batch_item_ids.append(item_id)\n",
    "                # batch_labels.append(label)\n",
    "                # batch_ratings.append(rating)\n",
    "            max_user_des = max(batch_num_items)\n",
    "            batch_user_descriptions = self.padding_user_description(batch_user_descriptions, mask, max_user_des)\n",
    "            # batch_data.append((np.array(batch_user_descriptions, dtype=np.int16),\n",
    "            #                    np.array(batch_user_ids, dtype=np.int16),\n",
    "            #                    np.array(batch_item_ids, dtype=np.int16),\n",
    "            #                    np.array(batch_num_items, dtype=np.int16),\n",
    "            #                    np.array(batch_labels, dtype=np.int8),\n",
    "            #                    np.array(batch_ratings, dtype=np.int8)\n",
    "            #                    ))\n",
    "            batch_data.append((batch_user_descriptions,\n",
    "                               # batch_user_ids,\n",
    "                               # batch_item_ids,\n",
    "                               # batch_num_items,\n",
    "                               # batch_labels,\n",
    "                               # batch_ratings\n",
    "                               ))\n",
    "\n",
    "        #             if i % 1000 == 0:\n",
    "        #                 print(f'done %d/%d iter' % (i, num_iter))\n",
    "        #             if i > 1000:\n",
    "        #                 break\n",
    "        return batch_data, num_iter\n",
    "\n",
    "    def get_user_description(self, rated_items, item_id):\n",
    "        if rated_items.__contains__(item_id):\n",
    "            rated_items.remove(item_id)\n",
    "        return list(rated_items)\n",
    "\n",
    "    def padding_user_description_2(self, batch_user_descriptions, mask, max_len):\n",
    "        for i in range(batch_user_descriptions.__len__()):\n",
    "            batch_user_descriptions[i].extend([mask] * (max_len - batch_user_descriptions[i].__len__()))\n",
    "        return batch_user_descriptions\n",
    "\n",
    "    def padding_user_description(self, batch_user_descriptions, mask, max_len):\n",
    "        result = np.zeros([len(batch_user_descriptions), max_len], dtype=np.int32) + mask\n",
    "        for idx, user_des in enumerate(batch_user_descriptions):\n",
    "            result[idx][0:len(user_des)] = user_des\n",
    "        return result\n",
    "\n",
    "    def padding_user_description_1(self, batch_user_descriptions, mask, max_len):\n",
    "        return np.array(list(itertools.zip_longest(*batch_user_descriptions, fillvalue=mask))).T\n",
    "\n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FISM(Model):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super(FISM, self).__init__()\n",
    "        self.embedding_size = args['embedding_size']\n",
    "        self.alpha = args['alpha']\n",
    "        self.beta = args['beta']\n",
    "        self.gamma = args['gamma']\n",
    "        self.lambda_ = args['lambda_']\n",
    "        self.verbose = args['verborse']\n",
    "        self.num_items = args['num_items']\n",
    "        self.num_users = args['num_users']\n",
    "        self.confidence_factor = args['confidence_factor']\n",
    "        self.Q_norms = None\n",
    "        self.P_norms = None\n",
    "        self.item_norms = None\n",
    "        self.item_vectors = None\n",
    "        self.P = tf.Variable(\n",
    "            tf.random.truncated_normal(shape=[self.num_items, self.embedding_size], mean=0, stddev=0.1))\n",
    "        self.mask_value = tf.constant(0, shape=(1, self.embedding_size), dtype=tf.float32)\n",
    "        self.Q = tf.Variable(\n",
    "            tf.random.truncated_normal(shape=[self.num_items, self.embedding_size], mean=0, stddev=0.1))\n",
    "        self.bias_u = tf.keras.layers.Embedding(input_dim=self.num_users, output_dim=1,\n",
    "                                                embeddings_initializer=TruncatedNormal(mean=0., stddev=0.1))\n",
    "        self.bias_i = tf.keras.layers.Embedding(input_dim=self.num_items, output_dim=1,\n",
    "                                                embeddings_initializer=TruncatedNormal(mean=0., stddev=0.1))\n",
    "\n",
    "    def call(self, user_descriptions, user_ids, item_ids, num_items):\n",
    "        user_bias = self.bias_u(user_ids)\n",
    "        item_bias = self.bias_i(item_ids)\n",
    "        P_with_mask = tf.concat([self.P, self.mask_value], axis=0)\n",
    "        user_rated_items_embedding = tf.nn.embedding_lookup(P_with_mask, user_descriptions)\n",
    "        items_embedding = tf.nn.embedding_lookup(self.Q, item_ids)\n",
    "        user_des = tf.reduce_sum(user_rated_items_embedding, axis=1)\n",
    "        coefficient = tf.pow(num_items, -tf.constant(self.alpha, dtype=tf.float32))\n",
    "        r = tf.squeeze(user_bias) + tf.squeeze(item_bias) + tf.math.multiply(coefficient, tf.reduce_sum(tf.math.multiply(user_des, items_embedding), axis=1))\n",
    "        return r\n",
    "\n",
    "    def loss_fn_old(self, predictions, labels, ratings):\n",
    "        confidences = 1 + self.confidence_factor * ratings\n",
    "        loss = tf.reduce_sum(tf.math.multiply(confidences, tf.math.square(predictions - labels)))\n",
    "        loss += self.beta * (tf.reduce_sum(tf.math.square(self.P)) + tf.reduce_sum(\n",
    "            tf.math.square(self.Q)))\n",
    "        loss += self.lambda_ * tf.reduce_sum(tf.math.square(self.bias_u.embeddings)) + self.gamma * tf.reduce_sum(\n",
    "            tf.math.square(self.bias_i.embeddings))\n",
    "        return loss\n",
    "\n",
    "    def loss_fn(self, predictions, labels, ratings):\n",
    "        predictions = tf.math.sigmoid(predictions)\n",
    "        predictions = tf.clip_by_value(predictions, clip_value_min=1e-7, clip_value_max=1 - 1e-7)\n",
    "        cross_entropy_elements = -(tf.math.multiply(labels, tf.math.log(predictions)) +\n",
    "                                   tf.math.multiply(1 - labels, tf.math.log(1 - predictions)))\n",
    "        confidences = 1 + self.confidence_factor * ratings\n",
    "        loss = tf.reduce_sum(tf.math.multiply(confidences, cross_entropy_elements))\n",
    "        loss += self.beta * (tf.reduce_sum(tf.math.square(self.P)) + tf.reduce_sum(tf.math.square(self.Q)))\n",
    "        loss += self.lambda_ * tf.reduce_sum(tf.math.square(self.bias_u.embeddings)) + self.gamma * tf.reduce_sum(\n",
    "            tf.math.square(self.bias_i.embeddings))\n",
    "        return loss\n",
    "\n",
    "    def prepare_for_prediction(self):\n",
    "        self.Q_norms = tf.sqrt(tf.reduce_sum(tf.square(self.Q), axis=1))\n",
    "        self.P_norms = tf.sqrt(tf.reduce_sum(tf.square(self.P), axis=1))\n",
    "        self.item_vectors = tf.concat([self.P, self.Q], axis=1)\n",
    "        self.item_norms = tf.sqrt(tf.reduce_sum(tf.square(self.item_vectors), axis=1))\n",
    "\n",
    "    def sim_items(self, item_id, top_n: int = 100):\n",
    "        item_embedded = tf.nn.embedding_lookup(self.P, item_id)\n",
    "        item_embedded = tf.reshape(item_embedded, shape=(self.embedding_size, -1))\n",
    "        scores = tf.matmul(self.Q, item_embedded)\n",
    "        scores = tf.squeeze(scores)\n",
    "        scores = scores / (self.Q_norms * self.P_norms[item_id])\n",
    "        scores = scores.numpy()\n",
    "        best = np.argpartition(scores, -top_n)[-top_n:]\n",
    "        return sorted(zip(best, scores[best]), key=lambda x: -x[1])\n",
    "\n",
    "    def sim_items_concat_pq(self, item_id, top_n: int = 100):\n",
    "        item_embedded = tf.nn.embedding_lookup(self.item_vectors, item_id)\n",
    "        item_embedded = tf.reshape(item_embedded, shape=(2 * self.embedding_size, -1))\n",
    "        scores = tf.matmul(self.item_vectors, item_embedded)\n",
    "        scores = tf.squeeze(scores)\n",
    "        scores = scores / (self.item_norms * self.item_norms[item_id])\n",
    "        scores = scores.numpy()\n",
    "        best = np.argpartition(scores, -top_n)[-top_n:]\n",
    "        return sorted(zip(best, scores[best]), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_top_n(model, user_id, user_rated_items, top_n=100, batch_size=512):\n",
    "    rated_items = set(user_rated_items[user_id])\n",
    "    predicts = []\n",
    "    user_descriptions = []\n",
    "    user_ids = []\n",
    "    item_ids = []\n",
    "    num_items = []\n",
    "    for item_id in range(model.num_items):\n",
    "        if rated_items.__contains__(item_id):\n",
    "            user_descriptions.append(list(rated_items.difference([item_id])) + [model.num_items])\n",
    "            user_ids.append(user_id)\n",
    "            item_ids.append(item_id)\n",
    "            num_items.append(rated_items.__len__() - 1)\n",
    "        else:\n",
    "            user_descriptions.append(list(rated_items.difference([item_id])))\n",
    "            user_ids.append(user_id)\n",
    "            item_ids.append(item_id)\n",
    "            num_items.append(rated_items.__len__())\n",
    "        if user_descriptions.__len__() >= batch_size:\n",
    "            batch_predict = model(np.array(user_descriptions, dtype=np.int32),\n",
    "                                  np.array(user_ids, dtype=np.int32),\n",
    "                                  np.array(item_ids, dtype=np.int32),\n",
    "                                  np.array(num_items, dtype=np.float32))\n",
    "            predicts += list(batch_predict.numpy())\n",
    "            user_descriptions = []\n",
    "            user_ids = []\n",
    "            item_ids = []\n",
    "            num_items = []\n",
    "    batch_predict = model(np.array(user_descriptions, dtype=np.int32),\n",
    "                          np.array(user_ids, dtype=np.int32),\n",
    "                          np.array(item_ids, dtype=np.int32),\n",
    "                          np.array(num_items, dtype=np.float32))\n",
    "    predicts += list(batch_predict.numpy())\n",
    "    items_score = [(iid, score) for iid, score in enumerate(predicts)]\n",
    "    items_score.sort(key=lambda x: x[1], reverse=True)\n",
    "    return items_score[:top_n]\n",
    "\n",
    "\n",
    "def hit_rate_evaluate(fism_model, user_rated_items, dataset):\n",
    "    total_items = 0\n",
    "    in_train_count = 0\n",
    "    count = 0\n",
    "    count_hit = 0\n",
    "    ndcg_users = []\n",
    "    for user_id, rated_items in tqdm(dataset.test_users.items()):\n",
    "        user_gains = []\n",
    "        rec_top_n = predict_top_n(fism_model, user_id, user_rated_items, batch_size=256, top_n=10)\n",
    "        top_item_ids = {rec_item[0] for rec_item in rec_top_n}\n",
    "        for position, item_id in enumerate(rated_items):\n",
    "            in_train_count += 1\n",
    "            if top_item_ids.__contains__(item_id):\n",
    "                count_hit += 1\n",
    "                user_gains.append(1 / np.log(position + 2))\n",
    "        idcg = 0\n",
    "        for i in range(user_gains.__len__()):\n",
    "            idcg += 1/np.log(i + 2)\n",
    "        if idcg > 0:\n",
    "            ndcg_users.append(sum(user_gains) / idcg)\n",
    "        total_items += rated_items.__len__()\n",
    "        count += 1\n",
    "        if count > 100:\n",
    "            break\n",
    "    in_train_rate = in_train_count / total_items\n",
    "    hit_rate = count_hit / total_items\n",
    "    ndcg = np.mean(ndcg_users)\n",
    "    return in_train_rate, hit_rate, ndcg\n",
    "\n",
    "\n",
    "def rank_score_evaluate(fism_model, user_rated_items, dataset):\n",
    "    count = 0\n",
    "    list_user_ranks = []\n",
    "    num_item = dataset.num_items\n",
    "    total_pred = 0\n",
    "    pred_hit = 0\n",
    "    for user_id, rated_items in tqdm(dataset.test_users.items()):\n",
    "        list_rec_items = predict_top_n(fism_model, user_id, user_rated_items, batch_size=256, top_n=-1)\n",
    "        rec_items_idx = {item_id: idx + 1 for idx, (item_id, score) in enumerate(list_rec_items)}\n",
    "        user_ranks = []\n",
    "        for item_id in rated_items:\n",
    "            total_pred += 1\n",
    "            if rec_items_idx.__contains__(item_id):\n",
    "                pred_rank = rec_items_idx[item_id] / num_item\n",
    "                user_ranks.append(pred_rank)\n",
    "        list_user_ranks.append(user_ranks)\n",
    "        count += 1\n",
    "        if count > 100:\n",
    "            break\n",
    "    rank_mean_users = []\n",
    "    for user_ranks in list_user_ranks:\n",
    "        if user_ranks.__len__() > 0:\n",
    "            rank_mean_users.append(np.mean(user_ranks))\n",
    "    return np.mean(rank_mean_users), pred_hit / total_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model, optimizer, user_descriptions, user_ids, item_ids, num_items, labels, ratings):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(user_descriptions, user_ids, item_ids, num_items)\n",
    "        loss = model.loss_fn(predictions, labels, ratings)\n",
    "    gradients = tape.gradient(target=loss, sources=model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def training(fism_model, optimizer, dataset, num_epochs, pretrained=False):\n",
    "    epoch_step = tf.Variable(0, dtype=tf.int32)\n",
    "    ckpt = tf.train.Checkpoint(fism_model=fism_model, epoch_step=epoch_step)\n",
    "    manager = tf.train.CheckpointManager(checkpoint=ckpt, directory='./fism_ckpt', max_to_keep=3)\n",
    "    if pretrained:\n",
    "        ckpt.restore(manager.latest_checkpoint)\n",
    "    user_rated_items = dataset.user_rated_items\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = tf.constant(0, tf.float32)\n",
    "        start_load_data = time()\n",
    "        dataset.prepare_train_data()\n",
    "        load_data_time = time() - start_load_data\n",
    "        # print('done load data: ', load_data_time)\n",
    "        start_train_time = time()\n",
    "        for i in tqdm(range(dataset.num_batch)):\n",
    "            user_descriptions, user_ids, item_ids, num_items, labels, ratings = dataset.get_batch(i)\n",
    "            loss_step = train_step(fism_model, optimizer, user_descriptions, user_ids, item_ids, num_items, labels, ratings)\n",
    "            train_loss += loss_step\n",
    "        train_time = time() - start_train_time\n",
    "        print('epoch: ', epoch, '. load data time: ', load_data_time, '. train time: ', train_time, '. train loss: ', train_loss.numpy() / (dataset.num_batch))\n",
    "        if epoch % 2 == 0:\n",
    "            fism_model.prepare_for_prediction()\n",
    "            in_train_rate, user_hit_rate, ndcg = hit_rate_evaluate(fism_model, user_rated_items, dataset)\n",
    "            user_rank_score, rank_in_train_set = rank_score_evaluate(fism_model, user_rated_items, dataset)\n",
    "\n",
    "            score = {'ndcg': ndcg,\n",
    "                     'cf_hit_rate': user_hit_rate,\n",
    "                     'cf_in_train_set_rate': in_train_rate,\n",
    "                     'cf_rank': user_rank_score}\n",
    "\n",
    "            print('epoch: {}, score: {}'.format(epoch, score))\n",
    "            ckpt.epoch_step.assign_add(epoch + 1)\n",
    "            manager.save()\n",
    "            print('done save at epoch: ', ckpt.epoch_step.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = 'Data/'\n",
    "data = DataSet(base_folder + 'train.csv', base_folder + 'test.csv', negative_sample=1, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dict()\n",
    "args['embedding_size'] = 50\n",
    "args['alpha'] = 0.8\n",
    "args['beta'] = 0.0005\n",
    "# args['gamma'] = 0.001\n",
    "# args['lambda_'] = 0.001\n",
    "args['gamma'] = 0.000\n",
    "args['lambda_'] = 0.000\n",
    "args['verborse'] = 1\n",
    "args['num_items'] = data.num_items\n",
    "args['num_users'] = data.num_users\n",
    "args['confidence_factor'] = 1\n",
    "\n",
    "fism = FISM(args)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5097d0260ef2456ab1f94bd8ad9e47e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3868.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/dm/phucdk/venv_rec_fism/lib/python3.5/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/opt/dm/phucdk/venv_rec_fism/lib/python3.5/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/opt/dm/phucdk/venv_rec_fism/lib/python3.5/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/opt/dm/phucdk/venv_rec_fism/lib/python3.5/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/opt/dm/phucdk/venv_rec_fism/lib/python3.5/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function train_step at 0x7f517d0c4730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/dm/phucdk/venv_rec_fism/lib/python3.5/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 10 calls to <function train_step at 0x7f517d0c4730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/dm/phucdk/venv_rec_fism/lib/python3.5/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/opt/dm/phucdk/venv_rec_fism/lib/python3.5/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/opt/dm/phucdk/venv_rec_fism/lib/python3.5/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/opt/dm/phucdk/venv_rec_fism/lib/python3.5/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/opt/dm/phucdk/venv_rec_fism/lib/python3.5/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/opt/dm/phucdk/venv_rec_fism/lib/python3.5/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/opt/dm/phucdk/venv_rec_fism/lib/python3.5/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/opt/dm/phucdk/venv_rec_fism/lib/python3.5/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/opt/dm/phucdk/venv_rec_fism/lib/python3.5/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/opt/dm/phucdk/venv_rec_fism/lib/python3.5/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/opt/dm/phucdk/venv_rec_fism/lib/python3.5/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/opt/dm/phucdk/venv_rec_fism/lib/python3.5/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/opt/dm/phucdk/venv_rec_fism/lib/python3.5/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/opt/dm/phucdk/venv_rec_fism/lib/python3.5/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/opt/dm/phucdk/venv_rec_fism/lib/python3.5/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch:  0 . load data time:  6.526012659072876 . train time:  220.43587493896484 . train loss:  381.10819544984486\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3da0ed9c7d544e3290ffa08fa735631f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3691.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac9da78053964e588299700e88ef819e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3691.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 0, score: {'cf_in_train_set_rate': 1.0, 'ndcg': 2.0813689810056077, 'cf_hit_rate': 0.03896103896103896, 'cf_rank': 0.07519410197237192}\n",
      "done save at epoch:  1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dca41a8c4fe4d069dd2655ac1ddcbbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3868.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/dm/phucdk/venv_rec_fism/lib/python3.5/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/opt/dm/phucdk/venv_rec_fism/lib/python3.5/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch:  1 . load data time:  6.6546196937561035 . train time:  213.3049738407135 . train loss:  338.001066442606\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bdfe05d1e8a4f2ea610a71379a93bc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3868.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/dm/phucdk/venv_rec_fism/lib/python3.5/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/opt/dm/phucdk/venv_rec_fism/lib/python3.5/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/opt/dm/phucdk/venv_rec_fism/lib/python3.5/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/opt/dm/phucdk/venv_rec_fism/lib/python3.5/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch:  2 . load data time:  6.559308767318726 . train time:  214.54083585739136 . train loss:  329.98516675284384\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d74916a786054e11b4cd616f95fb9797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3691.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34329f21e5cb428284ec079fbb7cca1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3691.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 2, score: {'cf_in_train_set_rate': 1.0, 'ndcg': 1.9736442398978449, 'cf_hit_rate': 0.047619047619047616, 'cf_rank': 0.07441752010293567}\n",
      "done save at epoch:  4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "084457ff239a4625ade1ecb8408f3ac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3868.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/dm/phucdk/venv_rec_fism/lib/python3.5/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch:  3 . load data time:  6.5346198081970215 . train time:  213.80501103401184 . train loss:  325.3194803516029\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cbede22bd7b49328924971f06d38a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3868.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/dm/phucdk/venv_rec_fism/lib/python3.5/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch:  4 . load data time:  6.847893714904785 . train time:  213.86710858345032 . train loss:  323.08111427094104\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f843f335fcf440883e20b19f362625f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3691.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71efcaa16e7d4e5a8fa329a5fe3b317d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3691.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 4, score: {'cf_in_train_set_rate': 1.0, 'ndcg': 2.0045518447327613, 'cf_hit_rate': 0.04329004329004329, 'cf_rank': 0.07468077463110682}\n",
      "done save at epoch:  9\n"
     ]
    }
   ],
   "source": [
    "training(fism, opt, data, num_epochs=100, pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
